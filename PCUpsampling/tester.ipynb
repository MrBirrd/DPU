{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DINOV2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Patch Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ[\"XFORMERS_DISABLED\"] = \"1\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import urllib\n",
    "\n",
    "import io\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def load_array_from_url(url: str) -> np.ndarray:\n",
    "    with urllib.request.urlopen(url) as f:\n",
    "        array_data = f.read()\n",
    "        g = io.BytesIO(array_data)\n",
    "        return np.load(g)\n",
    "\n",
    "\n",
    "def load_image_from_url(url: str) -> Image:\n",
    "    with urllib.request.urlopen(url) as f:\n",
    "        return Image.open(f).convert(\"RGB\")\n",
    "\n",
    "\n",
    "# Precomputed foreground / background projection\n",
    "STANDARD_ARRAY_URL = \"https://dl.fbaipublicfiles.com/dinov2/arrays/standard.npy\"\n",
    "standard_array = load_array_from_url(STANDARD_ARRAY_URL)\n",
    "\n",
    "EXAMPLE_IMAGE_URL = \"https://dl.fbaipublicfiles.com/dinov2/images/example.jpg\"\n",
    "example_image = load_image_from_url(EXAMPLE_IMAGE_URL)\n",
    "display(example_image)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_image.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from typing import Tuple\n",
    "from torch import Tensor\n",
    "from einops import rearrange\n",
    "\n",
    "\n",
    "def load_dino(cfg):\n",
    "    model = torch.hub.load(\"facebookresearch/dinov2\", cfg.model_name).cuda()\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def make_transform(smaller_edge_size: int) -> transforms.Compose:\n",
    "    IMAGENET_DEFAULT_MEAN = (0.485, 0.456, 0.406)\n",
    "    IMAGENET_DEFAULT_STD = (0.229, 0.224, 0.225)\n",
    "    interpolation_mode = transforms.InterpolationMode.BILINEAR\n",
    "\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize(size=smaller_edge_size, interpolation=interpolation_mode, antialias=True),\n",
    "        transforms.Normalize(mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD),\n",
    "    ])\n",
    "\n",
    "def prepare_image(image,\n",
    "                  smaller_edge_size: float,\n",
    "                  patch_size: int) -> Tuple[torch.Tensor, Tuple[int, int]]:\n",
    "    transform = make_transform(int(smaller_edge_size))\n",
    "\n",
    "    image_tensor = transform(image)\n",
    "    # Crop image to dimensions that are a multiple of the patch size\n",
    "    height, width = image_tensor.shape[1:] # C x H x W\n",
    "    cropped_width, cropped_height = width - width % patch_size, height - height % patch_size\n",
    "    image_tensor = image_tensor[:, :cropped_height, :cropped_width]\n",
    "\n",
    "    grid_size = (cropped_height // patch_size, cropped_width // patch_size) # h x w\n",
    "    return image_tensor, grid_size\n",
    "\n",
    "def get_dino_features(model, image, patch_size=14):\n",
    "    C, H, W = image.shape\n",
    "    interpolation = transforms.Resize(size=(H, W), interpolation=transforms.InterpolationMode.BILINEAR, antialias=True)\n",
    "    \n",
    "    smaller_edge_size = min(H, W)\n",
    "    image_tensor, grid_size = prepare_image(image, smaller_edge_size, patch_size)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        image_batch = image_tensor.cuda().unsqueeze(0)\n",
    "        t = model.get_intermediate_layers(image_batch)[0].squeeze().cpu()\n",
    "    \n",
    "    t_min = t.min(dim=0, keepdim=True).values\n",
    "    t_max = t.max(dim=0, keepdim=True).values\n",
    "    normalized_t = (t - t_min) / (t_max - t_min)\n",
    "    \n",
    "    features = normalized_t.reshape(*grid_size, -1)\n",
    "    features = rearrange(features, \"h w c -> 1 c h w\")\n",
    "    features = interpolation(features)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf, DictConfig\n",
    "\n",
    "cfg = {\"model_name\": \"dinov2_vits14\"}\n",
    "cfg = DictConfig(cfg)\n",
    "\n",
    "\n",
    "model = load_dino(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = torch.tensor(np.array(example_image) / 255.0).permute(2, 0, 1).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = get_dino_features(model, image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from PIL.Image import Resampling\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.ndimage import binary_closing, binary_opening\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "REPO_NAME = \n",
    "MODEL_NAME = \"dinov2_vits14\"\n",
    "\n",
    "\n",
    "DEFAULT_SMALLER_EDGE_SIZE = 448\n",
    "DEFAULT_BACKGROUND_THRESHOLD = -100\n",
    "DEFAULT_APPLY_OPENING = False\n",
    "DEFAULT_APPLY_CLOSING = False\n",
    "\n",
    "\n",
    "def make_transform(smaller_edge_size: int) -> transforms.Compose:\n",
    "    IMAGENET_DEFAULT_MEAN = (0.485, 0.456, 0.406)\n",
    "    IMAGENET_DEFAULT_STD = (0.229, 0.224, 0.225)\n",
    "    interpolation_mode = transforms.InterpolationMode.BICUBIC\n",
    "\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize(size=smaller_edge_size, interpolation=interpolation_mode, antialias=True),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD),\n",
    "    ])\n",
    "\n",
    "\n",
    "def prepare_image(image: Image,\n",
    "                  smaller_edge_size: float,\n",
    "                  patch_size: int) -> Tuple[torch.Tensor, Tuple[int, int]]:\n",
    "    transform = make_transform(int(smaller_edge_size))\n",
    "    image_tensor = transform(image)\n",
    "\n",
    "    # Crop image to dimensions that are a multiple of the patch size\n",
    "    height, width = image_tensor.shape[1:] # C x H x W\n",
    "    cropped_width, cropped_height = width - width % patch_size, height - height % patch_size\n",
    "    image_tensor = image_tensor[:, :cropped_height, :cropped_width]\n",
    "\n",
    "    grid_size = (cropped_height // patch_size, cropped_width // patch_size) # h x w (TODO: check)\n",
    "    return image_tensor, grid_size\n",
    "\n",
    "\n",
    "def make_foreground_mask(tokens,\n",
    "                         grid_size: Tuple[int, int],\n",
    "                         background_threshold: float = 0.0,\n",
    "                         apply_opening: bool = True,\n",
    "                         apply_closing: bool = True):\n",
    "    projection = tokens @ standard_array\n",
    "    mask = projection > background_threshold\n",
    "    mask = mask.reshape(*grid_size)\n",
    "    if apply_opening:\n",
    "        mask = binary_opening(mask)\n",
    "    if apply_closing:\n",
    "        mask = binary_closing(mask)\n",
    "    return mask.flatten()\n",
    "\n",
    "\n",
    "def render_patch_pca(image: Image,\n",
    "                     smaller_edge_size: float = 448,\n",
    "                     patch_size: int = 14,\n",
    "                     background_threshold: float = 0.05,\n",
    "                     apply_opening: bool = False,\n",
    "                     apply_closing: bool = False) -> Image:\n",
    "    image_tensor, grid_size = prepare_image(image, smaller_edge_size, patch_size)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        image_batch = image_tensor.unsqueeze(0).cuda()\n",
    "        tokens = model.get_intermediate_layers(image_batch)[0].squeeze().cpu()\n",
    "        print(tokens.shape)\n",
    "\n",
    "    if False:\n",
    "        mask = make_foreground_mask(tokens,\n",
    "                                    grid_size,\n",
    "                                    background_threshold,\n",
    "                                    apply_opening,\n",
    "                                    apply_closing)\n",
    "\n",
    "    pca = PCA(n_components=10)\n",
    "    #pca.fit(tokens[mask])\n",
    "    pca.fit(tokens)\n",
    "    projected_tokens = pca.transform(tokens)\n",
    "\n",
    "    t = torch.tensor(projected_tokens)\n",
    "    t_min = t.min(dim=0, keepdim=True).values\n",
    "    t_max = t.max(dim=0, keepdim=True).values\n",
    "    normalized_t = (t - t_min) / (t_max - t_min)\n",
    "\n",
    "    array = (normalized_t * 255).byte().numpy()\n",
    "    #array[~mask] = 0\n",
    "    print(array.shape, grid_size)\n",
    "    array = array[..., :3]\n",
    "    array = array.reshape(*grid_size, 3)\n",
    "\n",
    "    return Image.fromarray(array).resize((image.width, image.height), resample=Resampling.BICUBIC)\n",
    "\n",
    "\n",
    "\n",
    "print(f\"using {MODEL_NAME} model\")\n",
    "model = torch.hub.load(repo_or_dir=REPO_NAME, model=MODEL_NAME).cuda()\n",
    "model.eval()\n",
    "print(f\"patch size: {model.patch_size}\")\n",
    "\n",
    "render_patch_pca(image=example_image,\n",
    "                 smaller_edge_size=DEFAULT_SMALLER_EDGE_SIZE,\n",
    "                 patch_size=model.patch_size,\n",
    "                 background_threshold=DEFAULT_BACKGROUND_THRESHOLD,\n",
    "                 apply_opening=DEFAULT_APPLY_OPENING,\n",
    "                 apply_closing=DEFAULT_APPLY_CLOSING)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "32*42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dinov2_vits14 = torch.hub.load('facebookresearch/dinov2', 'dinov2_vits14').cuda().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img = torch.randn(1, 3, 1400, 1400).cuda()\n",
    "with torch.inference_mode():\n",
    "    out = dinov2_vits14.get_intermediate_layers(test_img)\n",
    "\n",
    "print(out[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "384**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    out = model(test_img, return_patches=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import itertools\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from third_party.dinov2.dinov2.eval.depth.models import build_depther\n",
    "\n",
    "class CenterPadding(torch.nn.Module):\n",
    "    def __init__(self, multiple):\n",
    "        super().__init__()\n",
    "        self.multiple = multiple\n",
    "\n",
    "    def _get_pad(self, size):\n",
    "        new_size = math.ceil(size / self.multiple) * self.multiple\n",
    "        pad_size = new_size - size\n",
    "        pad_size_left = pad_size // 2\n",
    "        pad_size_right = pad_size - pad_size_left\n",
    "        return pad_size_left, pad_size_right\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def forward(self, x):\n",
    "        pads = list(itertools.chain.from_iterable(self._get_pad(m) for m in x.shape[:1:-1]))\n",
    "        output = F.pad(x, pads)\n",
    "        return output\n",
    "\n",
    "\n",
    "def create_depther(cfg, backbone_model, backbone_size, head_type):\n",
    "    train_cfg = cfg.get(\"train_cfg\")\n",
    "    test_cfg = cfg.get(\"test_cfg\")\n",
    "    depther = build_depther(cfg.model, train_cfg=train_cfg, test_cfg=test_cfg)\n",
    "\n",
    "    depther.backbone.forward = partial(\n",
    "        backbone_model.get_intermediate_layers,\n",
    "        n=cfg.model.backbone.out_indices,\n",
    "        reshape=True,\n",
    "        return_class_token=cfg.model.backbone.output_cls_token,\n",
    "        norm=cfg.model.backbone.final_norm,\n",
    "    )\n",
    "\n",
    "    if hasattr(backbone_model, \"patch_size\"):\n",
    "        depther.backbone.register_forward_pre_hook(lambda _, x: CenterPadding(backbone_model.patch_size)(x[0]))\n",
    "\n",
    "    return depther"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BACKBONE_SIZE = \"small\" # in (\"small\", \"base\", \"large\" or \"giant\")\n",
    "\n",
    "\n",
    "backbone_archs = {\n",
    "    \"small\": \"vits14\",\n",
    "    \"base\": \"vitb14\",\n",
    "    \"large\": \"vitl14\",\n",
    "    \"giant\": \"vitg14\",\n",
    "}\n",
    "backbone_arch = backbone_archs[BACKBONE_SIZE]\n",
    "backbone_name = f\"dinov2_{backbone_arch}\"\n",
    "\n",
    "backbone_model = torch.hub.load(repo_or_dir=\"facebookresearch/dinov2\", model=backbone_name)\n",
    "backbone_model.eval()\n",
    "backbone_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "\n",
    "import mmcv\n",
    "from mmcv.runner import load_checkpoint\n",
    "\n",
    "\n",
    "def load_config_from_url(url: str) -> str:\n",
    "    with urllib.request.urlopen(url) as f:\n",
    "        return f.read().decode()\n",
    "\n",
    "\n",
    "HEAD_DATASET = \"nyu\" # in (\"nyu\", \"kitti\")\n",
    "HEAD_TYPE = \"dpt\" # in (\"linear\", \"linear4\", \"dpt\")\n",
    "\n",
    "\n",
    "DINOV2_BASE_URL = \"https://dl.fbaipublicfiles.com/dinov2\"\n",
    "head_config_url = f\"{DINOV2_BASE_URL}/{backbone_name}/{backbone_name}_{HEAD_DATASET}_{HEAD_TYPE}_config.py\"\n",
    "head_checkpoint_url = f\"{DINOV2_BASE_URL}/{backbone_name}/{backbone_name}_{HEAD_DATASET}_{HEAD_TYPE}_head.pth\"\n",
    "\n",
    "cfg_str = load_config_from_url(head_config_url)\n",
    "cfg = mmcv.utils.Config.fromstring(cfg_str, file_format=\".py\")\n",
    "\n",
    "model = create_depther(\n",
    "    cfg,\n",
    "    backbone_model=backbone_model,\n",
    "    backbone_size=BACKBONE_SIZE,\n",
    "    head_type=HEAD_TYPE,\n",
    ")\n",
    "\n",
    "load_checkpoint(model, head_checkpoint_url, map_location=\"cpu\")\n",
    "model.eval()\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.decode_head.fusion_blocks = torch.nn.Sequential(torch.nn.Identity())\n",
    "model.decode_head.project = torch.nn.Identity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img = torch.randn(1, 3, 224, 224).cuda()\n",
    "with torch.no_grad():\n",
    "    out = model.whole_inference(test_img, img_meta=None, rescale=True)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(out[0].cpu().squeeze().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def load_image_from_url(url: str) -> Image:\n",
    "    with urllib.request.urlopen(url) as f:\n",
    "        return Image.open(f).convert(\"RGB\")\n",
    "\n",
    "\n",
    "EXAMPLE_IMAGE_URL = \"https://dl.fbaipublicfiles.com/dinov2/images/example.jpg\"\n",
    "\n",
    "\n",
    "image = load_image_from_url(EXAMPLE_IMAGE_URL)\n",
    "display(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import matplotlib\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "def make_depth_transform() -> transforms.Compose:\n",
    "    return transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        lambda x: 255.0 * x[:3], # Discard alpha component and scale by 255\n",
    "        transforms.Normalize(\n",
    "            mean=(123.675, 116.28, 103.53),\n",
    "            std=(58.395, 57.12, 57.375),\n",
    "        ),\n",
    "    ])\n",
    "\n",
    "\n",
    "def render_depth(values, colormap_name=\"magma_r\") -> Image:\n",
    "    min_value, max_value = values.min(), values.max()\n",
    "    normalized_values = (values - min_value) / (max_value - min_value)\n",
    "\n",
    "    colormap = matplotlib.colormaps[colormap_name]\n",
    "    colors = colormap(normalized_values, bytes=True) # ((1)xhxwx4)\n",
    "    colors = colors[:, :, :3] # Discard alpha component\n",
    "    return Image.fromarray(colors)\n",
    "\n",
    "\n",
    "transform = make_depth_transform()\n",
    "\n",
    "scale_factor = 1\n",
    "rescaled_image = image.resize((scale_factor * image.width, scale_factor * image.height))\n",
    "transformed_image = transform(rescaled_image)\n",
    "batch = transformed_image.unsqueeze(0).cuda() # Make a batch of one image\n",
    "\n",
    "with torch.inference_mode():\n",
    "    result = model.whole_inference(batch, img_meta=None, rescale=True)\n",
    "\n",
    "depth_image = render_depth(result.squeeze().cpu())\n",
    "display(depth_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DINO V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import torchvision.transforms\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "import torch.nn.modules.utils as nn_utils\n",
    "import math\n",
    "import timm\n",
    "import types\n",
    "from pathlib import Path\n",
    "from typing import Union, List, Tuple\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "class ViTExtractor:\n",
    "    \"\"\" This class facilitates extraction of features, descriptors, and saliency maps from a ViT.\n",
    "\n",
    "    We use the following notation in the documentation of the module's methods:\n",
    "    B - batch size\n",
    "    h - number of heads. usually takes place of the channel dimension in pytorch's convention BxCxHxW\n",
    "    p - patch size of the ViT. either 8 or 16.\n",
    "    t - number of tokens. equals the number of patches + 1, e.g. HW / p**2 + 1. Where H and W are the height and width\n",
    "    of the input image.\n",
    "    d - the embedding dimension in the ViT.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_type: str = 'dino_vits8', stride: int = 4, model: nn.Module = None, device: str = 'cuda'):\n",
    "        \"\"\"\n",
    "        :param model_type: A string specifying the type of model to extract from.\n",
    "                          [dino_vits8 | dino_vits16 | dino_vitb8 | dino_vitb16 | vit_small_patch8_224 |\n",
    "                          vit_small_patch16_224 | vit_base_patch8_224 | vit_base_patch16_224]\n",
    "        :param stride: stride of first convolution layer. small stride -> higher resolution.\n",
    "        :param model: Optional parameter. The nn.Module to extract from instead of creating a new one in ViTExtractor.\n",
    "                      should be compatible with model_type.\n",
    "        \"\"\"\n",
    "        self.model_type = model_type\n",
    "        self.device = device\n",
    "        if model is not None:\n",
    "            self.model = model\n",
    "        else:\n",
    "            self.model = ViTExtractor.create_model(model_type)\n",
    "\n",
    "        self.model = ViTExtractor.patch_vit_resolution(self.model, stride=stride)\n",
    "        self.model.eval()\n",
    "        self.model.to(self.device)\n",
    "        self.p = self.model.patch_embed.patch_size\n",
    "        self.stride = self.model.patch_embed.proj.stride\n",
    "\n",
    "        self.mean = (0.485, 0.456, 0.406) if \"dino\" in self.model_type else (0.5, 0.5, 0.5)\n",
    "        self.std = (0.229, 0.224, 0.225) if \"dino\" in self.model_type else (0.5, 0.5, 0.5)\n",
    "\n",
    "        self._feats = []\n",
    "        self.hook_handlers = []\n",
    "        self.load_size = None\n",
    "        self.num_patches = None\n",
    "\n",
    "    @staticmethod\n",
    "    def create_model(model_type: str) -> nn.Module:\n",
    "        \"\"\"\n",
    "        :param model_type: a string specifying which model to load. [dino_vits8 | dino_vits16 | dino_vitb8 |\n",
    "                           dino_vitb16 | vit_small_patch8_224 | vit_small_patch16_224 | vit_base_patch8_224 |\n",
    "                           vit_base_patch16_224]\n",
    "        :return: the model\n",
    "        \"\"\"\n",
    "        if 'dino' in model_type:\n",
    "            model = torch.hub.load('facebookresearch/dino:main', model_type)\n",
    "        else:  # model from timm -- load weights from timm to dino model (enables working on arbitrary size images).\n",
    "            temp_model = timm.create_model(model_type, pretrained=True)\n",
    "            model_type_dict = {\n",
    "                'vit_small_patch16_224': 'dino_vits16',\n",
    "                'vit_small_patch8_224': 'dino_vits8',\n",
    "                'vit_base_patch16_224': 'dino_vitb16',\n",
    "                'vit_base_patch8_224': 'dino_vitb8'\n",
    "            }\n",
    "            model = torch.hub.load('facebookresearch/dino:main', model_type_dict[model_type])\n",
    "            temp_state_dict = temp_model.state_dict()\n",
    "            del temp_state_dict['head.weight']\n",
    "            del temp_state_dict['head.bias']\n",
    "            model.load_state_dict(temp_state_dict)\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def _fix_pos_enc(patch_size: int, stride_hw: Tuple[int, int]):\n",
    "        \"\"\"\n",
    "        Creates a method for position encoding interpolation.\n",
    "        :param patch_size: patch size of the model.\n",
    "        :param stride_hw: A tuple containing the new height and width stride respectively.\n",
    "        :return: the interpolation method\n",
    "        \"\"\"\n",
    "        def interpolate_pos_encoding(self, x: torch.Tensor, w: int, h: int) -> torch.Tensor:\n",
    "            npatch = x.shape[1] - 1\n",
    "            N = self.pos_embed.shape[1] - 1\n",
    "            if npatch == N and w == h:\n",
    "                return self.pos_embed\n",
    "            class_pos_embed = self.pos_embed[:, 0]\n",
    "            patch_pos_embed = self.pos_embed[:, 1:]\n",
    "            dim = x.shape[-1]\n",
    "            # compute number of tokens taking stride into account\n",
    "            w0 = 1 + (w - patch_size) // stride_hw[1]\n",
    "            h0 = 1 + (h - patch_size) // stride_hw[0]\n",
    "            assert (w0 * h0 == npatch), f\"\"\"got wrong grid size for {h}x{w} with patch_size {patch_size} and \n",
    "                                            stride {stride_hw} got {h0}x{w0}={h0 * w0} expecting {npatch}\"\"\"\n",
    "            # we add a small number to avoid floating point error in the interpolation\n",
    "            # see discussion at https://github.com/facebookresearch/dino/issues/8\n",
    "            w0, h0 = w0 + 0.1, h0 + 0.1\n",
    "            patch_pos_embed = nn.functional.interpolate(\n",
    "                patch_pos_embed.reshape(1, int(math.sqrt(N)), int(math.sqrt(N)), dim).permute(0, 3, 1, 2),\n",
    "                scale_factor=(w0 / math.sqrt(N), h0 / math.sqrt(N)),\n",
    "                mode='bicubic',\n",
    "                align_corners=False, recompute_scale_factor=False\n",
    "            )\n",
    "            assert int(w0) == patch_pos_embed.shape[-2] and int(h0) == patch_pos_embed.shape[-1]\n",
    "            patch_pos_embed = patch_pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n",
    "            return torch.cat((class_pos_embed.unsqueeze(0), patch_pos_embed), dim=1)\n",
    "\n",
    "        return interpolate_pos_encoding\n",
    "\n",
    "    @staticmethod\n",
    "    def patch_vit_resolution(model: nn.Module, stride: int) -> nn.Module:\n",
    "        \"\"\"\n",
    "        change resolution of model output by changing the stride of the patch extraction.\n",
    "        :param model: the model to change resolution for.\n",
    "        :param stride: the new stride parameter.\n",
    "        :return: the adjusted model\n",
    "        \"\"\"\n",
    "        patch_size = model.patch_embed.patch_size\n",
    "        if stride == patch_size:  # nothing to do\n",
    "            return model\n",
    "\n",
    "        stride = nn_utils._pair(stride)\n",
    "        assert all([(patch_size // s_) * s_ == patch_size for s_ in\n",
    "                    stride]), f'stride {stride} should divide patch_size {patch_size}'\n",
    "\n",
    "        # fix the stride\n",
    "        model.patch_embed.proj.stride = stride\n",
    "        # fix the positional encoding code\n",
    "        model.interpolate_pos_encoding = types.MethodType(ViTExtractor._fix_pos_enc(patch_size, stride), model)\n",
    "        return model\n",
    "\n",
    "    def preprocess(self, image: torch.Tensor,\n",
    "                   load_size: Union[int, Tuple[int, int]] = None) -> Tuple[torch.Tensor, Image.Image]:\n",
    "        \"\"\"\n",
    "        Preprocesses an image before extraction.\n",
    "        :param image_path: path to image to be extracted.\n",
    "        :param load_size: optional. Size to resize image before the rest of preprocessing.\n",
    "        :return: a tuple containing:\n",
    "                    (1) the preprocessed image as a tensor to insert the model of shape BxCxHxW.\n",
    "                    (2) the pil image in relevant dimensions\n",
    "        \"\"\"\n",
    "        # pil_image = image.convert('RGB')\n",
    "        # if load_size is not None:\n",
    "        #     pil_image = transforms.Resize(load_size, interpolation=transforms.InterpolationMode.LANCZOS)(pil_image)\n",
    "        prep = transforms.Compose([\n",
    "            # transforms.ToTensor(),\n",
    "            transforms.Resize(load_size, antialias=None),\n",
    "            transforms.Normalize(mean=self.mean, std=self.std)\n",
    "        ])\n",
    "        prep_img = prep(image)[None, ...]\n",
    "        return prep_img\n",
    "\n",
    "    def _get_hook(self, facet: str):\n",
    "        \"\"\"\n",
    "        generate a hook method for a specific block and facet.\n",
    "        \"\"\"\n",
    "        if facet in ['attn', 'token']:\n",
    "            def _hook(model, input, output):\n",
    "                self._feats.append(output)\n",
    "            return _hook\n",
    "\n",
    "        if facet == 'query':\n",
    "            facet_idx = 0\n",
    "        elif facet == 'key':\n",
    "            facet_idx = 1\n",
    "        elif facet == 'value':\n",
    "            facet_idx = 2\n",
    "        else:\n",
    "            raise TypeError(f\"{facet} is not a supported facet.\")\n",
    "\n",
    "        def _inner_hook(module, input, output):\n",
    "            input = input[0]\n",
    "            B, N, C = input.shape\n",
    "            qkv = module.qkv(input).reshape(B, N, 3, module.num_heads, C // module.num_heads).permute(2, 0, 3, 1, 4)\n",
    "            self._feats.append(qkv[facet_idx]) #Bxhxtxd\n",
    "        return _inner_hook\n",
    "\n",
    "    def _register_hooks(self, layers: List[int], facet: str) -> None:\n",
    "        \"\"\"\n",
    "        register hook to extract features.\n",
    "        :param layers: layers from which to extract features.\n",
    "        :param facet: facet to extract. One of the following options: ['key' | 'query' | 'value' | 'token' | 'attn']\n",
    "        \"\"\"\n",
    "        for block_idx, block in enumerate(self.model.blocks):\n",
    "            if block_idx in layers:\n",
    "                if facet == 'token':\n",
    "                    self.hook_handlers.append(block.register_forward_hook(self._get_hook(facet)))\n",
    "                elif facet == 'attn':\n",
    "                    self.hook_handlers.append(block.attn.attn_drop.register_forward_hook(self._get_hook(facet)))\n",
    "                elif facet in ['key', 'query', 'value']:\n",
    "                    self.hook_handlers.append(block.attn.register_forward_hook(self._get_hook(facet)))\n",
    "                else:\n",
    "                    raise TypeError(f\"{facet} is not a supported facet.\")\n",
    "\n",
    "    def _unregister_hooks(self) -> None:\n",
    "        \"\"\"\n",
    "        unregisters the hooks. should be called after feature extraction.\n",
    "        \"\"\"\n",
    "        for handle in self.hook_handlers:\n",
    "            handle.remove()\n",
    "        self.hook_handlers = []\n",
    "\n",
    "    def _extract_features(self, batch: torch.Tensor, layers: List[int] = 11, facet: str = 'key') -> List[torch.Tensor]:\n",
    "        \"\"\"\n",
    "        extract features from the model\n",
    "        :param batch: batch to extract features for. Has shape BxCxHxW.\n",
    "        :param layers: layer to extract. A number between 0 to 11.\n",
    "        :param facet: facet to extract. One of the following options: ['key' | 'query' | 'value' | 'token' | 'attn']\n",
    "        :return : tensor of features.\n",
    "                  if facet is 'key' | 'query' | 'value' has shape Bxhxtxd\n",
    "                  if facet is 'attn' has shape Bxhxtxt\n",
    "                  if facet is 'token' has shape Bxtxd\n",
    "        \"\"\"\n",
    "        B, C, H, W = batch.shape\n",
    "        self._feats = []\n",
    "        self._register_hooks(layers, facet)\n",
    "        _ = self.model(batch)\n",
    "        self._unregister_hooks()\n",
    "        self.load_size = (H, W)\n",
    "        self.num_patches = (1 + (H - self.p) // self.stride[0], 1 + (W - self.p) // self.stride[1])\n",
    "        return self._feats\n",
    "\n",
    "    def _log_bin(self, x: torch.Tensor, hierarchy: int = 2) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        create a log-binned descriptor.\n",
    "        :param x: tensor of features. Has shape Bxhxtxd.\n",
    "        :param hierarchy: how many bin hierarchies to use.\n",
    "        \"\"\"\n",
    "        B = x.shape[0]\n",
    "        num_bins = 1 + 8 * hierarchy\n",
    "\n",
    "        bin_x = x.permute(0, 2, 3, 1).flatten(start_dim=-2, end_dim=-1)  # Bx(t-1)x(dxh)\n",
    "        bin_x = bin_x.permute(0, 2, 1)\n",
    "        bin_x = bin_x.reshape(B, bin_x.shape[1], self.num_patches[0], self.num_patches[1])\n",
    "        # Bx(dxh)xnum_patches[0]xnum_patches[1]\n",
    "        sub_desc_dim = bin_x.shape[1]\n",
    "\n",
    "        avg_pools = []\n",
    "        # compute bins of all sizes for all spatial locations.\n",
    "        for k in range(0, hierarchy):\n",
    "            # avg pooling with kernel 3**kx3**k\n",
    "            win_size = 3 ** k\n",
    "            avg_pool = torch.nn.AvgPool2d(win_size, stride=1, padding=win_size // 2, count_include_pad=False)\n",
    "            avg_pools.append(avg_pool(bin_x))\n",
    "\n",
    "        bin_x = torch.zeros((B, sub_desc_dim * num_bins, self.num_patches[0], self.num_patches[1])).to(self.device)\n",
    "        for y in range(self.num_patches[0]):\n",
    "            for x in range(self.num_patches[1]):\n",
    "                part_idx = 0\n",
    "                # fill all bins for a spatial location (y, x)\n",
    "                for k in range(0, hierarchy):\n",
    "                    kernel_size = 3 ** k\n",
    "                    for i in range(y - kernel_size, y + kernel_size + 1, kernel_size):\n",
    "                        for j in range(x - kernel_size, x + kernel_size + 1, kernel_size):\n",
    "                            if i == y and j == x and k != 0:\n",
    "                                continue\n",
    "                            if 0 <= i < self.num_patches[0] and 0 <= j < self.num_patches[1]:\n",
    "                                bin_x[:, part_idx * sub_desc_dim: (part_idx + 1) * sub_desc_dim, y, x] = avg_pools[k][\n",
    "                                                                                                           :, :, i, j]\n",
    "                            else:  # handle padding in a more delicate way than zero padding\n",
    "                                temp_i = max(0, min(i, self.num_patches[0] - 1))\n",
    "                                temp_j = max(0, min(j, self.num_patches[1] - 1))\n",
    "                                bin_x[:, part_idx * sub_desc_dim: (part_idx + 1) * sub_desc_dim, y, x] = avg_pools[k][\n",
    "                                                                                                           :, :, temp_i,\n",
    "                                                                                                           temp_j]\n",
    "                            part_idx += 1\n",
    "        bin_x = bin_x.flatten(start_dim=-2, end_dim=-1).permute(0, 2, 1).unsqueeze(dim=1)\n",
    "        # Bx1x(t-1)x(dxh)\n",
    "        return bin_x\n",
    "\n",
    "    def extract_descriptors(self, batch: torch.Tensor, layer: List[int], facet: str = 'key',\n",
    "                            bin: bool = False, include_cls: bool = False) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        extract descriptors from the model\n",
    "        :param batch: batch to extract descriptors for. Has shape BxCxHxW.\n",
    "        :param layers: layer to extract. A number between 0 to 11.\n",
    "        :param facet: facet to extract. One of the following options: ['key' | 'query' | 'value' | 'token']\n",
    "        :param bin: apply log binning to the descriptor. default is False.\n",
    "        :return: tensor of descriptors. Bx1xtxd' where d' is the dimension of the descriptors.\n",
    "        \"\"\"\n",
    "        assert facet in ['key', 'query', 'value', 'token'], f\"\"\"{facet} is not a supported facet for descriptors. \n",
    "                                                             choose from ['key' | 'query' | 'value' | 'token'] \"\"\"\n",
    "        self._extract_features(batch, layer, facet)\n",
    "        x = torch.concat(self._feats)\n",
    "        # if facet == 'token':\n",
    "        #     x.unsqueeze_(dim=1) #Bx1xtxd\n",
    "        if not include_cls:\n",
    "            x = x[:, :, 1:, :]  # remove cls token\n",
    "        else:\n",
    "            assert not bin, \"bin = True and include_cls = True are not supported together, set one of them False.\"\n",
    "        if not bin:\n",
    "            desc = x.permute(0, 2, 3, 1).flatten(start_dim=-2, end_dim=-1).unsqueeze(dim=1)  # Bx1xtx(dxh)\n",
    "        else:\n",
    "            desc = self._log_bin(x)\n",
    "        return desc\n",
    "\n",
    "    def extract_saliency_maps(self, batch: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        extract saliency maps. The saliency maps are extracted by averaging several attention heads from the last layer\n",
    "        in of the CLS token. All values are then normalized to range between 0 and 1.\n",
    "        :param batch: batch to extract saliency maps for. Has shape BxCxHxW.\n",
    "        :return: a tensor of saliency maps. has shape Bxt-1\n",
    "        \"\"\"\n",
    "        assert self.model_type == \"dino_vits8\", f\"saliency maps are supported only for dino_vits model_type.\"\n",
    "        self._extract_features(batch, [11], 'attn')\n",
    "        head_idxs = [0, 2, 4, 5]\n",
    "        curr_feats = self._feats[0] #Bxhxtxt\n",
    "        cls_attn_map = curr_feats[:, head_idxs, 0, 1:].mean(dim=1) #Bx(t-1)\n",
    "        temp_mins, temp_maxs = cls_attn_map.min(dim=1)[0], cls_attn_map.max(dim=1)[0]\n",
    "        cls_attn_maps = (cls_attn_map - temp_mins) / (temp_maxs - temp_mins)  # normalize to range [0,1]\n",
    "        return cls_attn_maps\n",
    "\n",
    "\"\"\" taken from https://stackoverflow.com/questions/15008758/parsing-boolean-values-with-argparse\"\"\"\n",
    "def str2bool(v):\n",
    "    if isinstance(v, bool):\n",
    "        return v\n",
    "    if v.lower() in ('yes', 'true', 't', 'y', '1'):\n",
    "        return True\n",
    "    elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n",
    "        return False\n",
    "    else:\n",
    "        raise argparse.ArgumentTypeError('Boolean value expected.')\n",
    "\n",
    "'''\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description='Facilitate ViT Descriptor extraction.')\n",
    "    parser.add_argument('--image_path', type=str, required=True, help='path of the extracted image.')\n",
    "    parser.add_argument('--output_path', type=str, required=True, help='path to file containing extracted descriptors.')\n",
    "    parser.add_argument('--load_size', default=224, type=int, help='load size of the input image.')\n",
    "    parser.add_argument('--stride', default=4, type=int, help=\"\"\"stride of first convolution layer. \n",
    "                                                              small stride -> higher resolution.\"\"\")\n",
    "    parser.add_argument('--model_type', default='dino_vits8', type=str,\n",
    "                        help=\"\"\"type of model to extract. \n",
    "                        Choose from [dino_vits8 | dino_vits16 | dino_vitb8 | dino_vitb16 | vit_small_patch8_224 | \n",
    "                        vit_small_patch16_224 | vit_base_patch8_224 | vit_base_patch16_224]\"\"\")\n",
    "    parser.add_argument('--facet', default='key', type=str, help=\"\"\"facet to create descriptors from. \n",
    "                                                                    options: ['key' | 'query' | 'value' | 'token']\"\"\")\n",
    "    parser.add_argument('--layer', default=11, type=int, help=\"layer to create descriptors from.\")\n",
    "    parser.add_argument('--bin', default='False', type=str2bool, help=\"create a binned descriptor if True.\")\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        extractor = ViTExtractor(args.model_type, args.stride, device=device)\n",
    "        image_batch, image_pil = extractor.preprocess(args.image_path, args.load_size)\n",
    "        print(f\"Image {args.image_path} is preprocessed to tensor of size {image_batch.shape}.\")\n",
    "        descriptors = extractor.extract_descriptors(image_batch.to(device), args.layer, args.facet, args.bin)\n",
    "        print(f\"Descriptors are of size: {descriptors.shape}\")\n",
    "        torch.save(descriptors, args.output_path)\n",
    "        print(f\"Descriptors saved to: {args.output_path}\")\n",
    "\n",
    "    import pdb; pdb.set_trace()\n",
    "    print()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model_type = 'dino_vitb8'\n",
    "stride = 4\n",
    "load_size = 224\n",
    "test_image = torch.randn(3, 224, 224).cuda()\n",
    "layer = [11]\n",
    "facet = 'key'\n",
    "bin = False\n",
    "\n",
    "extractor = ViTExtractor(model_type, stride, device=device)\n",
    "image_batch = extractor.preprocess(test_image, load_size)\n",
    "print(f\"Image is preprocessed to tensor of size {image_batch.shape}.\")\n",
    "descriptors = extractor.extract_descriptors(image_batch.to(device), layer, facet, bin)\n",
    "descriptors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2809*384"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def extract_feature(model, frame, return_h_w=False):\n",
    "    \"\"\"Extract one frame feature everytime.\"\"\"\n",
    "    out = model.get_intermediate_layers(frame.unsqueeze(0).cuda(), n=1)[0]\n",
    "    out = out[:, 1:, :]  # we discard the [CLS] token\n",
    "    h, w = int(frame.shape[1] / model.patch_embed.patch_size), int(frame.shape[2] / model.patch_embed.patch_size)\n",
    "    dim = out.shape[-1]\n",
    "    out = out[0].reshape(h, w, dim)\n",
    "    #out = out.reshape(-1, dim)\n",
    "    #if return_h_w:\n",
    "    #    return out, h, w\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vits16 = torch.hub.load('facebookresearch/dino:main', 'dino_vits16').cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OPENCLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import open_clip\n",
    "import torch\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, _, preprocess = open_clip.create_model_and_transforms('ViT-B-32', pretrained='laion2b_s34b_b79k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.token_embedding = torch.nn.Identity()\n",
    "model.ln_final = torch.nn.Identity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(torch.randn(1, 3, 224, 224))[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = preprocess(Image.open(\"../room.jpeg\")).unsqueeze(0)\n",
    "\n",
    "with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "    image_features = model.encode_image(image)\n",
    "    #image_features /= image_features.norm(dim=-1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Feature Lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from third_party.scannetpp.common.utils.colmap import read_model\n",
    "from third_party.scannetpp.common.scene_release import ScannetppScene_Release\n",
    "import open3d as o3d\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_id = \"036bce3393\"\n",
    "data_root = \"../datasets/scannetpp/\"\n",
    "movie_path = \"../datasets/scannetpp/036bce3393/iphone/rgb.mp4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene = ScannetppScene_Release(scene_id, data_root=data_root)\n",
    "mesh_path = scene.scan_mesh_path\n",
    "\n",
    "colmap_dir = scene.iphone_colmap_dir\n",
    "cameras, images, points3D = read_model(colmap_dir, \".txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh = o3d.io.read_triangle_mesh(str(mesh_path))\n",
    "pointcloud = o3d.geometry.PointCloud(mesh.vertices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add black color to pointcloud\n",
    "pointcloud.colors = o3d.utility.Vector3dVector(np.zeros_like(mesh.vertices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from decord import VideoReader\n",
    "from decord import cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the mp4\n",
    "import cv2\n",
    "\n",
    "vr = VideoReader(movie_path, ctx=cpu(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_frames = 10\n",
    "frame_idxs = np.linspace(0, vr._num_frame//skip_frames * skip_frames ,round(vr._num_frame//skip_frames+1), dtype=np.int32)\n",
    "videoframes = vr.get_batch(frame_idxs).asnumpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def project_point_cloud(points, translation, rotation, camera_matrix):\n",
    "    # Assuming points is already a NumPy array\n",
    "    points_transformed = np.dot(rotation, points.T) + translation\n",
    "    points_projected = np.dot(camera_matrix, points_transformed)\n",
    "    points_projected[:2, :] /= points_projected[2, :]\n",
    "    return points_projected.T\n",
    "\n",
    "def filter_points(points_projected, image_width, image_height, min_depth=0.1, max_depth=1000):\n",
    "    # Check within image bounds and positive depth\n",
    "    in_image_bounds = (points_projected[:, 0] >= 0) & (points_projected[:, 0] < image_width) & \\\n",
    "                      (points_projected[:, 1] >= 0) & (points_projected[:, 1] < image_height)\n",
    "    valid_depth = (points_projected[:, 2] > min_depth) & (points_projected[:, 2] < max_depth)\n",
    "    valid_indices = in_image_bounds & valid_depth\n",
    "    return valid_indices\n",
    "\n",
    "def map_image_features_to_ptc(image, valid_points_2d, valid_indices):\n",
    "    # Assuming valid_points_2d is a NumPy array\n",
    "    x, y = valid_points_2d[:, 0].astype(int), valid_points_2d[:, 1].astype(int)\n",
    "    rgb_values = image[y, x]  # Efficient bulk operation\n",
    "    return rgb_values, valid_indices\n",
    "\n",
    "def update_features(feature_list, feature_values, valid_indices):\n",
    "    # convert valid_indices to indexes\n",
    "    valid_indices = np.where(valid_indices)[0]\n",
    "    for feature, valid_index in zip(feature_values, valid_indices):\n",
    "        feature_list[valid_index].append(feature)\n",
    "    return feature_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iphone_intrinsics_path = scene.iphone_pose_intrinsic_imu_path\n",
    "iphone_intrinsics = json.load(open(iphone_intrinsics_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_width = 1920\n",
    "image_height = 1440"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = np.array(pointcloud.points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptc_points = [[] for _ in range(len(points))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_idx, image in tqdm(images.items()):\n",
    "        if image_idx % 5 != 0:\n",
    "            continue\n",
    "        world_to_camera = image.world_to_camera\n",
    "        camera_to_world = np.linalg.inv(world_to_camera)\n",
    "        \n",
    "        # plot axis\n",
    "        axis = o3d.geometry.TriangleMesh.create_coordinate_frame(size=0.2)\n",
    "        axis.transform(camera_to_world)\n",
    "        #vis.add_geometry(axis)\n",
    "        \n",
    "        # extract video frame\n",
    "        frame_name = image.name\n",
    "        frame = int(frame_name.split('_')[-1].split('.')[0]) // 10\n",
    "        videoframe = videoframes[frame] / 255.0\n",
    "        \n",
    "        # project the mesh on the camera and extract the rgb values from the videoframe\n",
    "        iphone_data = iphone_intrinsics[frame_name.split('.')[0]]\n",
    "        intrinsic_matrix = iphone_data['intrinsic']\n",
    "        \n",
    "        R = world_to_camera[:3, :3]\n",
    "        t = world_to_camera[:-1, -1:]\n",
    "        \n",
    "        # project points\n",
    "        points_projected = project_point_cloud(points, t, R, intrinsic_matrix)\n",
    "        valid_indices = filter_points(points_projected, image_width, image_height)\n",
    "        rgb_feats, valid_indices = map_image_features_to_ptc(videoframe, points_projected[valid_indices], valid_indices)\n",
    "        update_features(ptc_points, rgb_feats, valid_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_idx = []\n",
    "\n",
    "for idx, f in tqdm(enumerate(ptc_points)):\n",
    "    if len(f) > 0:\n",
    "        f = np.array(f)\n",
    "        ptc_points[idx] = np.median(f, axis=0)\n",
    "    else:\n",
    "        ptc_points[idx] = np.zeros(3)\n",
    "        missing_idx.append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptcf = np.array(ptc_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptcf_copy = ptcf.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for every missing idx do a knn search and replace the value\n",
    "from scipy.spatial import KDTree\n",
    "\n",
    "tree = KDTree(ptcf)\n",
    "for idx in tqdm(missing_idx):\n",
    "    dist, ind = tree.query([ptcf[idx]], k=20)\n",
    "    neighbors = ptcf[ind[0]]\n",
    "    neighbors = neighbors[neighbors[:, 0] != 0]\n",
    "    ptcf[idx] = np.mean(neighbors, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.asarray(pointcloud.colors)[:,:] = ptcf_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis = o3d.visualization.Visualizer()\n",
    "vis.create_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis.add_geometry(pointcloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis.poll_events()\n",
    "vis.update_renderer()\n",
    "vis.run()\n",
    "vis.destroy_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(filtered.astype(np.int32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import open3d as o3d\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"scannet_cut_small_st\": \"ST\",\n",
    "    \"scannet_cut_small_pvd_st\": \"PVD + ST\",\n",
    "    \"scannet_cut_small_pvd_large_mse\": \"PVD\",\n",
    "    \"scannet_cut_small_mink\": \"Mink\"\n",
    "}\n",
    "\n",
    "ckpt = \"99999\"\n",
    "config = \"DDPM(T=200)_clip\"\n",
    "idx = \"000\"\n",
    "\n",
    "data = {}\n",
    "\n",
    "for model in models:\n",
    "    path = os.path.join(\"checkpoints\", model, \"sampling\", ckpt, config)\n",
    "    gt_path = os.path.join(path, idx+\"_gt_highres.npy\")\n",
    "    hints_path = os.path.join(path, idx+\"_hints.npy\")\n",
    "    pred_path = os.path.join(path, idx+\"_pred.npy\")\n",
    "\n",
    "    gt = np.load(gt_path)\n",
    "    hints = np.load(hints_path)\n",
    "    pred = np.load(pred_path)\n",
    "    new_data = {\n",
    "        \"gt\": gt,\n",
    "        \"hints\": hints,\n",
    "        \"pred\": pred\n",
    "    }\n",
    "    data[models[model]] = new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "green = np.array([0, 255, 0])\n",
    "red = np.array([255, 0, 0])\n",
    "black = np.array([0, 0, 0])\n",
    "blue = np.array([0, 0, 255])\n",
    "orange = np.array([255, 165, 0])\n",
    "colors = [red, black, blue, orange]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange, repeat\n",
    "import pyviz3d.visualizer as viz\n",
    "\n",
    "v = viz.Visualizer()\n",
    "\n",
    "pointsize = 3\n",
    "for idx, model in enumerate(data):\n",
    "    pred = data[model][\"pred\"].swapaxes(1, 2)\n",
    "    hints = data[model][\"hints\"].swapaxes(1, 2)\n",
    "    gt = data[model][\"gt\"].swapaxes(1, 2)\n",
    "    B, N, C = pred.shape\n",
    "\n",
    "    col = colors[idx]\n",
    "    for batch_idx in range(pred.shape[0]):\n",
    "        v.add_points(point_size=pointsize, positions=pred[batch_idx], colors=repeat(col, 'C -> N C', C=C, N=N), name=model+\"_pred_\"+str(batch_idx), visible=False)\n",
    "        v.add_points(point_size=pointsize, positions=hints[batch_idx], colors=repeat(col, 'C -> N C', C=C, N=N), name=model+\"_hints_\"+str(batch_idx), visible=False)\n",
    "        v.add_points(point_size=pointsize, positions=gt[batch_idx], colors=repeat(col, 'C -> N C', C=C, N=N), name=model+\"_gt_\"+str(batch_idx), visible=False)\n",
    "\n",
    "v.save(\"plot_superviz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from model.unet_mink import MinkUnet\n",
    "from model.unet_pointvoxel import PVCLionSmall\n",
    "\n",
    "pvd_model = \"checkpoints/scannet_cut_pvd/step_199999.pth\"\n",
    "pvd_model = torch.load(pvd_model, map_location=torch.device('cpu'))\n",
    "pvd_conf = OmegaConf.load(\"checkpoints/scannet_cut_pvd/opt.yaml\")\n",
    "\n",
    "\n",
    "mink_model = \"checkpoints/scannet_cut_mink/step_199999.pth\"\n",
    "mink_model = torch.load(mink_model, map_location=torch.device('cpu'))\n",
    "mink_conf = OmegaConf.load(\"checkpoints/scannet_cut_mink/opt.yaml\")\n",
    "\n",
    "cfg = pvd_conf\n",
    "pvd_model = PVCLionSmall(out_dim=cfg.model.out_dim,\n",
    "                input_dim=cfg.model.in_dim,\n",
    "                npoints=cfg.data.npoints,\n",
    "                embed_dim=cfg.model.time_embed_dim,\n",
    "                use_att=cfg.model.use_attention,\n",
    "                dropout=cfg.model.dropout,\n",
    "                extra_feature_channels=cfg.model.extra_feature_channels)\n",
    "cfg = mink_conf\n",
    "mink_model = MinkUnet(dim=cfg.model.time_embed_dim,\n",
    "                init_ds_factor=cfg.model.Mink.init_ds_factor,\n",
    "                D=cfg.model.Mink.D,\n",
    "                in_shape=[cfg.training.bs, cfg.model.in_dim, cfg.data.npoints],\n",
    "                out_dim=cfg.model.out_dim,\n",
    "                in_channels=cfg.model.in_dim + cfg.model.extra_feature_channels,\n",
    "                dim_mults=cfg.model.Mink.dim_mults,\n",
    "                downsampfactors=cfg.model.Mink.downsampfactors,\n",
    "                use_attention=cfg.model.use_attention,)\n",
    "\n",
    "# print number of parameters in Millions\n",
    "print(\"Number of parameters in PVD model: \", sum(p.numel() for p in pvd_model.parameters())/1000000)\n",
    "print(\"Number of parameters in Mink model: \", sum(p.numel() for p in mink_model.parameters())/1000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "stats_paths = glob(\"checkpoints/**/stats.csv\", recursive=True)\n",
    "\n",
    "stats = []\n",
    "\n",
    "for path in stats_paths:\n",
    "    df = pd.read_csv(path)\n",
    "    if \"model\" in df.columns:\n",
    "        stats.append(df)\n",
    "\n",
    "stats = pd.concat(stats)\n",
    "stats.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter the experiments out\n",
    "stats_scannet_small = stats[stats.model.str.contains(\"scannet_cut_small\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mapping = {\n",
    "    \"scannet_cut_pvd_att\": \"PVD + ATT\",\n",
    "    \"scannet_cut_pvd\": \"PVD\",\n",
    "    \"scannet_cut_mink\": \"Mink\",\n",
    "    \"scannet_cut_st\": \"ST\",\n",
    "    \"scannet_cut_small_pvd_large_mse\": \"PVD\",\n",
    "    \"scannet_cut_small_mink\": \"Mink\",\n",
    "    \"scannet_cut_small_st\": \"ST\",\n",
    "    \"scannet_cut_small_pvd_st\": \"PVD + ST\",\n",
    "}\n",
    "stats['model'] = stats['model'].map(model_mapping)\n",
    "stats_scannet_small['model'] = stats_scannet_small['model'].map(model_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import scienceplots\n",
    "plt.style.use(['science', 'ieee'])\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_scannet_small = stats_scannet_small[stats_scannet_small.ckpt == 99999]\n",
    "stats_scannet_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = stats_scannet_small\n",
    "\n",
    "plt.figure(figsize=(6, 3))\n",
    "sns.lineplot(data=df, x=\"t\", y=\"cd\", hue=\"model\", style=\"model\", markers=True, legend=\"brief\")\n",
    "plt.xlabel(\"Sampling timesteps\")\n",
    "plt.ylabel(\"Chamfer Distance\")\n",
    "plt.title(\"Chamfer Distance vs Sampling Timesteps\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6, 3))\n",
    "sns.lineplot(data=df, x=\"t\", y=\"emd\", hue=\"model\", style=\"model\", markers=True, legend=\"brief\")\n",
    "plt.xlabel(\"Sampling timesteps\")\n",
    "plt.ylabel(\"EMD\")\n",
    "plt.title(\"EMD vs Sampling Timesteps\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate table for paper\n",
    "df = stats_scannet_small\n",
    "df = df[df.t == 200]\n",
    "df = df.groupby(\"model\").mean()\n",
    "df = df[[\"cd\", \"emd\"]]\n",
    "df = df.round(4)\n",
    "df = df.rename(columns={\"cd\": \"CD\", \"emd\": \"EMD\"})\n",
    "df = df.reset_index()\n",
    "\n",
    "print(df.to_markdown(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = stats[stats['ckpt'] == 299999]\n",
    "#df = stats[stats['ckpt'] >= 300000]\n",
    "#df = stats\n",
    "\n",
    "plt.figure(figsize=(6, 3))\n",
    "sns.lineplot(data=df, x=\"t\", y=\"cd\", hue=\"model\", style=\"model\", markers=True, legend=\"brief\")\n",
    "plt.xlabel(\"Sampling timesteps\")\n",
    "plt.ylabel(\"Chamfer Distance\")\n",
    "plt.title(\"Chamfer Distance vs Sampling Timesteps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = stats[stats['ckpt'] == 299999]\n",
    "\n",
    "plt.figure(figsize=(6, 3))\n",
    "sns.lineplot(data=df, x=\"t\", y=\"emd\", hue=\"model\", style=\"model\", markers=True, legend=\"brief\")\n",
    "plt.xlabel(\"Sampling timesteps\")\n",
    "plt.ylabel(\"EMD\")\n",
    "plt.title(\"EMD vs Sampling Timesteps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "def cosine_beta_schedule(timesteps, s=0.008):\n",
    "    steps = timesteps + 1\n",
    "    t = torch.linspace(0, timesteps, steps, dtype=torch.float64) / timesteps\n",
    "    alphas_cumprod = torch.cos((t + s) / (1 + s) * math.pi * 0.5) ** 2\n",
    "    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
    "    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
    "    return torch.clip(betas, 0, 0.999)\n",
    "\n",
    "# Calculate SNR for each timestep\n",
    "timesteps = 1000\n",
    "betas = cosine_beta_schedule(timesteps)\n",
    "alphas = 1 - betas\n",
    "alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
    "snr = alphas_cumprod / (1 - alphas_cumprod)\n",
    "\n",
    "# Convert SNR to a more readable format (numpy array)\n",
    "snr_numpy = snr.numpy()\n",
    "\n",
    "plt.figure(figsize=(6, 3))\n",
    "plt.plot(snr_numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pyviz3d.visualizer as viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptc_batch = \"checkpoints/scannet_cut_mink/sampling/299999/DDPM(T=20)_clip_dynamic/008_pred.npy\"\n",
    "ptc_gt_batch = \"checkpoints/scannet_cut_mink/sampling/299999/DDPM(T=20)_clip_dynamic/008_gt_highres.npy\"\n",
    "\n",
    "\n",
    "idx = \"005\"\n",
    "model = \"scannet_cut_small_pvd_large_mse\"\n",
    "\n",
    "ptc_batch = \"checkpoints/\"+model+\"/sampling/99999/DDPM(T=200)_clip/\" + idx + \"_pred.npy\"\n",
    "ptc_gt_batch = \"checkpoints/\"+model+\"/sampling/99999/DDPM(T=200)_clip/\" + idx + \"_gt_highres.npy\"\n",
    "ptc_sampling_path = \"checkpoints/\"+model+\"/sampling/99999/DDPM(T=200)_clip/\" + idx + \"_pred_all.npy\"\n",
    "\n",
    "#ptc_batch = \"checkpoints/scannet_cut_st/sampling/162499/DDPM(T=20)_clip_dynamic/001_pred.npy\"\n",
    "#ptc_gt_batch = \"checkpoints/scannet_cut_st/sampling/162499/DDPM(T=20)_clip_dynamic/001_gt_highres.npy\"\n",
    "\n",
    "ptc_batch = np.load(ptc_batch, allow_pickle=True)\n",
    "gt_batch = np.load(ptc_gt_batch, allow_pickle=True)\n",
    "ptc_sampling_batch = np.load(ptc_sampling_path, allow_pickle=True)\n",
    "\n",
    "end_sample = ptc_sampling_batch[-1]\n",
    "start_sample = ptc_sampling_batch[0]\n",
    "\n",
    "ps = 2\n",
    "v = viz.Visualizer()\n",
    "\n",
    "v.add_points(\"start\", start_sample.T, point_size=ps, visible=False)\n",
    "v.add_points(\"end\", end_sample.T, point_size=ps, visible=False)\n",
    "\n",
    "for idx, ptc in enumerate(ptc_batch):\n",
    "    v.add_points(f\"{idx}\", ptc.T, point_size=ps, visible=False)\n",
    "    v.add_points(f\"{idx}_gt\", gt_batch[idx].T, point_size=ps, visible=False)\n",
    "\n",
    "\n",
    "v.save(\"./plot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.loss import get_loss\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = get_loss(\"emd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt = torch.randn(4, 8192, 3).cuda()\n",
    "pred = torch.randn(4, 8192, 3).cuda()\n",
    "loss(gt, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Small Noise Schedules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_beta_schedule(timesteps, s=0.008):\n",
    "    \"\"\"\n",
    "    cosine schedule\n",
    "    as proposed in https://openreview.net/forum?id=-NEXDKk8gZ\n",
    "    \"\"\"\n",
    "    steps = timesteps + 1\n",
    "    t = torch.linspace(0, timesteps, steps, dtype=torch.float64) / timesteps\n",
    "    alphas_cumprod = torch.cos((t + s) / (1 + s) * math.pi * 0.5) ** 2\n",
    "    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
    "    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
    "    return torch.clip(betas, 0, 0.999)\n",
    "\n",
    "# truncated beta schedule which does not map from 1 to 0 but from 1 to constant c\n",
    "def truncated_beta_schedule(timesteps, s=0.008, c=0.5):\n",
    "    \"\"\"\n",
    "    truncated cosine schedule\n",
    "    as proposed in https://openreview.net/forum?id=-NEXDKk8gZ\n",
    "    \"\"\"\n",
    "    steps = timesteps + 1\n",
    "    t = torch.linspace(0, timesteps, steps, dtype=torch.float64) / timesteps\n",
    "    alphas_cumprod = torch.cos((t + s) / (1 + s) * math.pi * 0.5) ** 2\n",
    "    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
    "    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
    "    betas = torch.clip(betas, 0, 0.999)\n",
    "    betas = betas * c + (1 - c)\n",
    "    return betas\n",
    "\n",
    "def sigmoid_beta_schedule(timesteps, beta_start=0.0001, beta_end=0.02, a=10, b=-5):\n",
    "    \"\"\"\n",
    "    Generate a sigmoid-shaped beta schedule for diffusion process.\n",
    "\n",
    "    Args:\n",
    "    - timesteps (int): Number of timesteps in the diffusion process.\n",
    "    - beta_start (float): Starting noise level.\n",
    "    - beta_end (float): Maximum noise level.\n",
    "    - a (float): Steepness parameter for sigmoid function.\n",
    "    - b (float): Center shift parameter for sigmoid function.\n",
    "\n",
    "    Returns:\n",
    "    - numpy array: Array of beta values for each timestep.\n",
    "    \"\"\"\n",
    "    t = np.linspace(0, timesteps, timesteps, dtype=np.float64)\n",
    "    sigmoid_function = 1 / (1 + np.exp(-a * (t / timesteps) + b))\n",
    "    beta_schedule = beta_start + (beta_end - beta_start) * sigmoid_function\n",
    "    return np.clip(beta_schedule, 0, 0.999)  # Clipping to avoid very high values\n",
    "\n",
    "def linear_beta_schedule(timesteps, beta_start=0.0001, beta_end=0.02):\n",
    "    \"\"\"\n",
    "    linear schedule, proposed in original ddpm paper\n",
    "    \"\"\"\n",
    "    scale = 1000 / timesteps\n",
    "    beta_start = scale * beta_start\n",
    "    beta_end = scale * beta_end\n",
    "    return torch.linspace(beta_start, beta_end, timesteps, dtype=torch.float64)\n",
    "\n",
    "def calculate_snr(beta_schedule):\n",
    "    alphas = 1 - beta_schedule\n",
    "    alphas_cumprod = np.cumprod(alphas)\n",
    "    snr = alphas_cumprod / (1 - alphas_cumprod)\n",
    "    return snr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "betas = cosine_beta_schedule(200)\n",
    "alphas = 1 - betas\n",
    "\n",
    "plt.figure(figsize=(6, 3))\n",
    "plt.plot(alphas.numpy())\n",
    "plt.plot(betas.numpy())\n",
    "plt.legend([\"alpha\", \"beta\"])\n",
    "\n",
    "plt.figure(figsize=(6, 3))\n",
    "plt.plot(calculate_snr(betas))\n",
    "plt.title(\"SNR\")\n",
    "print(calculate_snr(betas)[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "betas = linear_beta_schedule(200, beta_start=1e-5, beta_end=1e-5)\n",
    "alphas = 1 - betas\n",
    "\n",
    "plt.figure(figsize=(6, 3))\n",
    "plt.plot(alphas.numpy())\n",
    "plt.plot(betas.numpy())\n",
    "plt.legend([\"alpha\", \"beta\"])\n",
    "\n",
    "plt.figure(figsize=(6, 3))\n",
    "plt.plot(calculate_snr(betas))\n",
    "plt.title(\"SNR\")\n",
    "print(calculate_snr(betas)[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "betas = sigmoid_beta_schedule(200, beta_start=1e-5, beta_end=1e-2, a=100, b=100)\n",
    "alphas = 1 - betas\n",
    "\n",
    "plt.figure(figsize=(9, 9))\n",
    "plt.plot(alphas)\n",
    "plt.plot(betas)\n",
    "plt.legend([\"alpha\", \"beta\"])\n",
    "\n",
    "plt.figure(figsize=(6, 3))\n",
    "plt.plot(calculate_snr(betas))\n",
    "plt.title(\"SNR\")\n",
    "print(calculate_snr(betas)[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PVD In Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.unet_pointvoxel import PVCNN2Unet\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import open3d as o3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npoints = 8192\n",
    "n_centers = [npoints//4, npoints//4**2, npoints//4**3, npoints//4**4]\n",
    "n_centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PVCAdaptive(PVCNN2Unet):\n",
    "    def __init__(\n",
    "        self,\n",
    "        out_dim: int = 3,\n",
    "        input_dim: int = 3,\n",
    "        embed_dim: int = 64,\n",
    "        npoints: int = 2048,\n",
    "        use_att: bool = True,\n",
    "        use_st: bool = False,\n",
    "        dropout: float = 0.1,\n",
    "        extra_feature_channels: int = 3,\n",
    "        width_multiplier: int = 1,\n",
    "        voxel_resolution_multiplier: int = 1,\n",
    "        self_cond: bool = False,\n",
    "    ):\n",
    "        voxel_resolutions = [32, 16, 8, 8]\n",
    "        n_sa_blocks = [2, 2, 3, 4]\n",
    "        n_fp_blocks = [2, 2, 3, 4]\n",
    "        n_centers = [npoints//4, npoints//4**2, npoints//4**3, npoints//4**4]\n",
    "        radius = [0.1, 0.2, 0.4, 0.8]\n",
    "        \n",
    "        sa_blocks = [\n",
    "            # conv vfg  , sa config\n",
    "            # out channels, num blocks, voxel resolution | num_centers, radius, num_neighbors, out_channels\n",
    "            ((32,  n_sa_blocks[0], voxel_resolutions[0]),   (n_centers[0], radius[0], 32, (32, 64))),\n",
    "            ((64,  n_sa_blocks[1], voxel_resolutions[1]),   (n_centers[1], radius[1], 32, (64, 128))),\n",
    "            ((128, n_sa_blocks[2], voxel_resolutions[2]),   (n_centers[2], radius[2], 32, (128, 256))),\n",
    "            (None,                                          (n_centers[3], radius[3], 32, (256, 256, 512))),\n",
    "        ]\n",
    "\n",
    "        # in_channels, out_channels X | out_channels, num_blocks, voxel_resolution\n",
    "        fp_blocks = [\n",
    "            ((256, 256),     (256, n_fp_blocks[3], voxel_resolutions[3])),\n",
    "            ((256, 256),     (256, n_fp_blocks[2], voxel_resolutions[2])),\n",
    "            ((256, 128),     (128, n_fp_blocks[1], voxel_resolutions[1])),\n",
    "            ((128, 128, 64), (64,  n_fp_blocks[0], voxel_resolutions[0])),\n",
    "        ]\n",
    "\n",
    "        super().__init__(\n",
    "            out_dim=out_dim,\n",
    "            input_dim=input_dim,\n",
    "            embed_dim=embed_dim,\n",
    "            use_att=use_att,\n",
    "            use_st=use_st,\n",
    "            dropout=dropout,\n",
    "            sa_blocks=sa_blocks,\n",
    "            fp_blocks=fp_blocks,\n",
    "            extra_feature_channels=extra_feature_channels,\n",
    "            width_multiplier=width_multiplier,\n",
    "            voxel_resolution_multiplier=voxel_resolution_multiplier,\n",
    "            self_cond=self_cond,\n",
    "            flash=False,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PVCAdaptive(out_dim=3, extra_feature_channels=0, npoints=8192, use_att=True, use_st=False).cuda()\n",
    "# print model parameters\n",
    "print(\"Number of parameters in PVC model: \", sum(p.numel() for p in model.parameters())/1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "def get_neighbor_index(vertices: \"(bs, vertice_num, 3)\", neighbor_num: int):\n",
    "    \"\"\"\n",
    "    Return: (bs, vertice_num, neighbor_num)\n",
    "    \"\"\"\n",
    "    bs, v, _ = vertices.size()\n",
    "    device = vertices.device\n",
    "    inner = torch.bmm(vertices, vertices.transpose(1, 2))  # (bs, v, v)\n",
    "    quadratic = torch.sum(vertices**2, dim=2)  # (bs, v)\n",
    "    inner.mul_(-2) \n",
    "    inner.add_(quadratic.unsqueeze(1)).add_(quadratic.unsqueeze(2))\n",
    "    neighbor_index = torch.topk(inner, k=neighbor_num + 1, dim=-1, largest=False)[1]\n",
    "    neighbor_index = neighbor_index[:, :, 1:]\n",
    "    return neighbor_index\n",
    "\n",
    "def indexing_neighbor(tensor: \"(bs, vertice_num, dim)\",\n",
    "                      index: \"(bs, vertice_num, neighbor_num)\"):\n",
    "    \"\"\"\n",
    "    Return: (bs, vertice_num, neighbor_num, dim)\n",
    "    \"\"\"\n",
    "    bs, v, n = index.size()\n",
    "    id_0 = torch.arange(bs).view(-1, 1, 1)\n",
    "    tensor_indexed = tensor[id_0, index]\n",
    "    return tensor_indexed\n",
    "\n",
    "class Pooling(nn.Module):\n",
    "    def __init__(self, pooling_rate: int = 4, neighbor_num: int = 4):\n",
    "        super().__init__()\n",
    "        self.pooling_rate = pooling_rate\n",
    "        self.neighbor_num = neighbor_num\n",
    "\n",
    "    def forward(self, vertices: \"(bs, vertice_num, 3)\",\n",
    "                feature_map: \"(bs, vertice_num, channel_num)\"):\n",
    "        \"\"\"\n",
    "        Return:\n",
    "            vertices_pool: (bs, pool_vertice_num, 3),\n",
    "            feature_map_pool: (bs, pool_vertice_num, channel_num)\n",
    "        \"\"\"\n",
    "        bs, vertice_num, _ = vertices.size()\n",
    "        neighbor_index = get_neighbor_index(vertices, self.neighbor_num)\n",
    "        neighbor_feature = indexing_neighbor(\n",
    "            feature_map,\n",
    "            neighbor_index)  \n",
    "        # (bs, vertice_num, neighbor_num, channel_num)\n",
    "        pooled_feature = torch.max(neighbor_feature, dim=2)[0]  \n",
    "        # (bs, vertice_num, channel_num)\n",
    "\n",
    "        pool_num = int(vertice_num / self.pooling_rate)\n",
    "        sample_idx = torch.randperm(vertice_num)[:pool_num]\n",
    "        vertices_pool = vertices[:, sample_idx, :]  \n",
    "        # (bs, pool_num, 3)\n",
    "        feature_map_pool = pooled_feature[:, sample_idx, :]  \n",
    "        # (bs, pool_num, channel_num)\n",
    "        return vertices_pool, feature_map_pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool_1 = Pooling(pooling_rate=4, neighbor_num=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertices = test_cloud.permute(0, 2, 1)\n",
    "fm = test_cloud.permute(0, 2, 1)\n",
    "test_cloud.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_ds, f_ds = pool_1(vertices, fm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertices_anchor = vertices\n",
    "neighbor_index = get_neighbor_index(vertices, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyminiply\n",
    "import scipy\n",
    "from einops import repeat\n",
    "\n",
    "test_room = \"../datasets/scannetpp/0a7cc12c0e/scans/mesh_aligned_0.05.ply\"\n",
    "ply, *_ = pyminiply.read(test_room)\n",
    "pcd_tree = scipy.spatial.cKDTree(ply)\n",
    "points = pcd_tree.data\n",
    "\n",
    "rand_idx = np.random.randint(0, len(points))\n",
    "rand_point = points[rand_idx]\n",
    "_, idx = pcd_tree.query(rand_point, k=8192, p=2)\n",
    "points = points[idx]\n",
    "\n",
    "# normalize\n",
    "points = points - points.mean(axis=0)\n",
    "points = points / points.max()\n",
    "points = points.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cloud = torch.from_numpy(points).unsqueeze(0).cuda().float()\n",
    "test_timestep = torch.rand(1).cuda()\n",
    "test_cloud.shape, test_timestep.shape\n",
    "\n",
    "B = 32\n",
    "test_cloud = repeat(test_cloud, 'b c n -> (repeat b) c n', repeat=B)\n",
    "test_timestep = repeat(test_timestep, 'b -> (repeat b)', repeat=B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_out = model(test_cloud, test_timestep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyviz3d.visualizer as viz\n",
    "from einops import repeat\n",
    "\n",
    "v = viz.Visualizer()\n",
    "v.add_points(\"cloud\", points.T, point_size=10)\n",
    "for idx, coords in enumerate(coords_list):\n",
    "    cloud = coords.detach().squeeze().cpu().numpy().T\n",
    "    color = np.random.rand(3)\n",
    "    coords_color = repeat(color, \"c -> n c\", n=cloud.shape[0])\n",
    "    v.add_points(f\"coords_{idx}\", cloud, colors=coords_color, point_size=10)\n",
    "v.save(\"./plot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minkowski in Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from minkowski.common import conv, conv_tr\n",
    "from einops import rearrange\n",
    "import MinkowskiEngine as ME\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_parse_tensor(coords, feats=None, voxel_size=0.001):\n",
    "    \"\"\"\n",
    "    Converts the input coordinates and features to a sparse tensor.\n",
    "\n",
    "    Args:\n",
    "        coords (torch.Tensor): The input coordinates.\n",
    "        feats (torch.Tensor): The input features.\n",
    "        voxel_size (float): The voxel size for scaling the coordinates.\n",
    "\n",
    "    Returns:\n",
    "        ME.SparseTensor: The sparse tensor with the converted coordinates and features.\n",
    "    \"\"\"\n",
    "\n",
    "    # switch from channel first to channel last\n",
    "    coords = coords.permute(0, 2, 1)\n",
    "    b, n, c = coords.shape\n",
    "\n",
    "    # scale for discrete coordinates\n",
    "    coords = coords / voxel_size\n",
    "\n",
    "    if feats is None:\n",
    "        feats = torch.zeros((b * n, 3)).to(coords.device)\n",
    "    if feats.ndim == 3:\n",
    "        assert feats.shape[-1] == n, \"feats must be of shape (b, c, n)\"\n",
    "        feats = rearrange(feats, \"b c n -> (b n) c\")\n",
    "\n",
    "    stensor = ME.SparseTensor(\n",
    "        features=feats,  # Convert to a tensor\n",
    "        coordinates=ME.utils.batched_coordinates(\n",
    "            [c for c in coords], dtype=torch.float32, device=coords.device\n",
    "        ),  # coordinates must be defined in a integer grid. If the scale\n",
    "        quantization_mode=ME.SparseTensorQuantizationMode.UNWEIGHTED_AVERAGE,  # when used with continuous coordinates, average features in the same coordinate\n",
    "    )\n",
    "    return stensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = torch.randn(1, 3, 8192).cuda()\n",
    "test_s = to_parse_tensor(test)\n",
    "\n",
    "\n",
    "class MinkTest(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv = conv(in_planes=3, out_planes=32, kernel_size=3, stride=1, dilation=1, bias=False, D=3)\n",
    "        self.conv_ds = conv(in_planes=32, out_planes=32, kernel_size=3, stride=2, dilation=1, bias=False, D=3)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.conv_ds(x)\n",
    "        return x\n",
    "\n",
    "model = MinkTest().cuda()\n",
    "out = model(test_s)\n",
    "out.F.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.diffusion_lucid import GaussianDiffusion\n",
    "from omegaconf import OmegaConf\n",
    "import numpy as np\n",
    "import open3d as o3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import point_cloud_utils as pcu\n",
    "import pyminiply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ply_path = \"../datasets/scannetpp/036bce3393/scans/mesh_aligned_0.05.ply\"\n",
    "#ply_path = \"../datasets/arkit/421378.ply\"\n",
    "#ply, *_ = pyminiply.read(ply_path)\n",
    "ply = np.array(o3d.io.read_point_cloud(ply_path).points)\n",
    "ply.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# voxel downsample\n",
    "pcd = pcu.downsample_point_cloud_on_voxel_grid(0.005, ply)\n",
    "pcd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "pcd_tree = scipy.spatial.cKDTree(pcd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_idx = np.random.randint(0, len(ply))   \n",
    "\n",
    "k_nearest = pcd_tree.query(ply[rand_idx], 8192)[1]\n",
    "x = pcd[k_nearest]\n",
    "\n",
    "#nearest_ball = pcd_tree.query_ball_point(ply[rand_idx], 1)\n",
    "#x = pcd[nearest_ball]\n",
    "\n",
    "print(x.shape)\n",
    "# plot point cloud\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(x[:, 0], x[:, 1], x[:, 2], s=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "cfg_path = \"checkpoints/scannet_cut_small_pvd_large_mse/opt.yaml\"\n",
    "cfg = OmegaConf.load(cfg_path)\n",
    "\n",
    "diffusion = GaussianDiffusion(cfg).cuda()\n",
    "diffusion.model = nn.parallel.DataParallel(diffusion.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = \"checkpoints/scannet_cut_small_pvd_large_mse/step_299999.pth\"\n",
    "ckpt = torch.load(ckpt_path)\n",
    "diffusion.load_state_dict(ckpt[\"model_state\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = x \n",
    "x0 = x0 - np.min(x0, axis=0)\n",
    "x0 = x0 / np.max(x0, axis=0)\n",
    "x0 = torch.from_numpy(x0).float().unsqueeze(0)\n",
    "x0 = x0.permute(0, 2, 1).cuda()\n",
    "samples = torch.tensor([])\n",
    "epsilon = torch.randn_like(x0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample, hint= diffusion.sample(shape=x0.shape, hint=x0, return_noised_hint=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"sample_pvd_300k.npy\", sample.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 20))\n",
    "ptc = hint.squeeze().detach().cpu().numpy().T\n",
    "\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "ax.scatter(ptc[:, 0], ptc[:, 1], ptc[:, 2], s=1)\n",
    "plt.savefig(\"hint.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyviz3d.visualizer import Visualizer\n",
    "\n",
    "folder = \"visualization/baselines_8k/\"\n",
    "items = os.listdir(folder)\n",
    "\n",
    "viz = Visualizer()\n",
    "\n",
    "for item in items:\n",
    "    ptc = np.load(folder + item).squeeze().T\n",
    "    viz.add_points(name=item, positions=ptc, point_size=2, visible=False)\n",
    "\n",
    "viz.save(\"./plot_baselines_8k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = x \n",
    "x0 = x0 - np.min(x0, axis=0)\n",
    "x0 = x0 / np.max(x0, axis=0)\n",
    "x0 = torch.from_numpy(x0).float().unsqueeze(0)\n",
    "samples = torch.tensor([])\n",
    "epsilon = torch.randn_like(x0)\n",
    "\n",
    "for t in np.arange(0, 200, 200//5):\n",
    "    t = torch.tensor(t).reshape(1,).long()\n",
    "    qsample = diffusion.q_sample(x0, t=t, noise=epsilon)\n",
    "    samples = torch.cat((samples, qsample), dim=0)\n",
    "\n",
    "t = torch.tensor(199).reshape(1,).long()\n",
    "qsample = diffusion.q_sample(x0, t=t, noise=epsilon)\n",
    "samples = torch.cat((samples, qsample), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate snr for the timesteps\n",
    "\n",
    "betas = diffusion.betas\n",
    "alphas = 1 - betas\n",
    "\n",
    "alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
    "snr = alphas_cumprod / (1 - alphas_cumprod)\n",
    "snr_db = 10 * torch.log10(snr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot all the samples in 3d next to each other\n",
    "\n",
    "fig = plt.figure(figsize=(20, 20))\n",
    "for idx, sample in enumerate(samples):\n",
    "    ax = fig.add_subplot(1, 6, idx+1, projection='3d')\n",
    "    ax.scatter(sample[:, 0], sample[:, 1], sample[:, 2], s=1)\n",
    "    ax.set_title(f\"t={idx*200//5}\\t snr=\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "diffusion = GaussianDiffusion(cfg)\n",
    "samples = torch.tensor([])\n",
    "tglobal = cfg.diffusion.timesteps\n",
    "\n",
    "\n",
    "x0 = x \n",
    "x0 = x0 - np.min(x0, axis=0)\n",
    "x0 = x0 / np.max(x0, axis=0)\n",
    "x0 = torch.from_numpy(x0).float().unsqueeze(0)\n",
    "\n",
    "epsilon = torch.randn_like(x0)\n",
    "\n",
    "for t in np.arange(0, tglobal, tglobal//10):\n",
    "    t = torch.tensor(t).reshape(1,).long()\n",
    "    qsample = diffusion.q_sample(x0, t=t, noise=epsilon)\n",
    "    samples = torch.cat((samples, qsample), dim=0)\n",
    "\n",
    "# plot samples next to each other\n",
    "fig = plt.figure(figsize=(20, 10))\n",
    "\n",
    "# set title\n",
    "fig.suptitle(f\"timesteps: {tglobal}\", fontsize=16)\n",
    "for i in range(10):\n",
    "    samp = samples[i, :, :].detach().numpy()\n",
    "    ax = fig.add_subplot(2, 5, i+1, projection='3d')\n",
    "    ax.scatter(samp[:, 0], samp[:, 1], samp[:, 2], s=1)\n",
    "    # calculate mean and var\n",
    "    mean = np.mean(samp)\n",
    "    var = np.var(samp)\n",
    "    # set title\n",
    "    ax.set_title(f\"timestep: {i * tglobal//10} mean: {mean:.2f}, var: {var:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# plot influence of denoising timesteps\n",
    "timesteps = [10, 50, 100]\n",
    "\n",
    "# get x0 and normalize to [-1, 1]\n",
    "x0 = x \n",
    "x0 = x0 - np.min(x0, axis=0)\n",
    "x0 = x0 / np.max(x0, axis=0)\n",
    "x0 = torch.from_numpy(x0).float().unsqueeze(0)\n",
    "\n",
    "epsilon = torch.randn_like(x0)\n",
    "\n",
    "\n",
    "for tglobal in timesteps:\n",
    "    cfg.diffusion.num_timesteps = tglobal\n",
    "    diffusion = GaussianDiffusion(cfg)\n",
    "    samples = torch.tensor([])\n",
    "    \n",
    "    for t in np.arange(0, tglobal, tglobal//10):\n",
    "        t = torch.tensor(t).reshape(1,).long()\n",
    "        qsample = diffusion.q_sample(x0, t=t, noise=epsilon)\n",
    "        samples = torch.cat((samples, qsample), dim=0)\n",
    "    \n",
    "    # plot samples next to each other\n",
    "    fig = plt.figure(figsize=(20, 10))\n",
    "    \n",
    "    # set title\n",
    "    fig.suptitle(f\"timesteps: {tglobal}\", fontsize=16)\n",
    "    for i in range(10):\n",
    "        samp = samples[i, :, :].detach().numpy()\n",
    "        ax = fig.add_subplot(2, 5, i+1, projection='3d')\n",
    "        ax.scatter(samp[:, 0], samp[:, 1], samp[:, 2], s=1)\n",
    "        # calculate mean and var\n",
    "        mean = np.mean(samp)\n",
    "        var = np.var(samp)\n",
    "        # set title\n",
    "        ax.set_title(f\"timestep: {i * tglobal//10} mean: {mean:.2f}, var: {var:.2f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.dataloader import get_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf\n",
    "\n",
    "cfg_path = \"configs/pvd_arkit.yml\"\n",
    "cfg = OmegaConf.load(cfg_path)\n",
    "cfg.data.data_dir = \"../datasets/scannetpp/\"\n",
    "cfg.data.dataset = \"ScanNetPP\"\n",
    "cfg.distribution_type = \"single\"\n",
    "cfg.training.bs = 1\n",
    "\n",
    "train_loader, val_loader, *_ = get_dataloader(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(dataloader))\n",
    "points = batch[\"train_points\"]\n",
    "\n",
    "# visualize point cloud\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(points[0, :, 0], points[0, :, 1], points[0, :, 2], s=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.dataloader import get_dataloader\n",
    "from omegaconf import OmegaConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = \"configs/overfit_room.yml\"\n",
    "opt = OmegaConf.load(conf)\n",
    "opt.data.dataset = \"Indoor\"\n",
    "opt.data.npoints = 2**14\n",
    "opt.distribution_type = \"single\"\n",
    "dl, *_ = get_dataloader(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter((dl)))\n",
    "data = batch[\"train_points\"]\n",
    "lowres = batch[\"train_points_lowres\"]\n",
    "data.shape, data.min(), data.max(), data.mean(), data.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyviz3d.visualizer as viz\n",
    "\n",
    "ps = 2\n",
    "\n",
    "v = viz.Visualizer()\n",
    "v.add_points(\"hires\", data[1, ...].numpy(), point_size=ps, visible=False)\n",
    "v.add_points(\"lowres\", lowres[1, ...].numpy(), point_size=ps, visible=True)\n",
    "v.save(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.utils import concat_nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "x1 = torch.randn(100000, 3)\n",
    "x2 = torch.randn(100000, 3)\n",
    "y = concat_nn(x1, x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PVCLion(out_dim=3, input_dim=3, embed_dim=16, npoints=1_000_000, use_att=False, extra_feature_channels=3, dropout=0.1).cuda()\n",
    "# print number of parameters\n",
    "print(f\"Number of parameters: {sum(p.numel() for p in model.parameters()) / 1_000_000}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DotDict(dict):\n",
    "    \"\"\"\n",
    "    a dictionary that supports dot notation \n",
    "    as well as dictionary access notation \n",
    "    usage: d = DotDict() or d = DotDict({'val1':'first'})\n",
    "    set attributes: d.val2 = 'second' or d['val2'] = 'second'\n",
    "    get attributes: d.val2 or d['val2']\n",
    "    \"\"\"\n",
    "    __getattr__ = dict.__getitem__\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__\n",
    "\n",
    "    def __init__(self, dct):\n",
    "        for key, value in dct.items():\n",
    "            if hasattr(value, 'keys'):\n",
    "                value = DotDict(value)\n",
    "            self[key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from minkowski.minkunet import ResUNet14, TinyUnet\n",
    "\n",
    "config = DotDict\n",
    "config.bn_momentum = 0.9\n",
    "config.conv1_kernel_size = 7\n",
    "\n",
    "model = TinyUnet(in_channels=3, out_channels=3, config=config, D=3).cuda()\n",
    "print(f\"Number of parameters: {sum(p.numel() for p in model.parameters()) / 1_000_000}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = IndoorScenes(root_dir=\"../../3d/\", npoints=1_000_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ds[0]\n",
    "\n",
    "pcond = data[\"train_points_lowres\"].unsqueeze(0).cuda()\n",
    "noise = torch.randn(1, 1_000_000, 3).cuda()\n",
    "t = torch.rand(1).cuda()\n",
    "\n",
    "# concat\n",
    "#x = torch.cat([pcond, noise], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MinkowskiEngine as ME\n",
    "\n",
    "def to_parse_tensor(coords, feats=None, voxel_size=0.001):\n",
    "    \"\"\"\n",
    "    Converts the input coordinates and features to a sparse tensor.\n",
    "\n",
    "    Args:\n",
    "        coords (torch.Tensor): The input coordinates.\n",
    "        feats (torch.Tensor): The input features.\n",
    "        voxel_size (float): The voxel size for scaling the coordinates.\n",
    "\n",
    "    Returns:\n",
    "        ME.SparseTensor: The sparse tensor with the converted coordinates and features.\n",
    "    \"\"\"\n",
    "    # scaling\n",
    "    coords = coords / voxel_size\n",
    "    b, n, c = coords.shape\n",
    "    \n",
    "    if feats is None:\n",
    "        feats = torch.zeros((b*n, 3)).to(coords.device)\n",
    "    elif feats.ndim == 3:\n",
    "        feats = feats.reshape(b*n, 3)\n",
    "\n",
    "    stensor = ME.SparseTensor(\n",
    "        features=feats, # Convert to a tensor\n",
    "        coordinates=ME.utils.batched_coordinates([c for c in coords], dtype=torch.float32, device=coords.device),  # coordinates must be defined in a integer grid. If the scale\n",
    "        quantization_mode=ME.SparseTensorQuantizationMode.UNWEIGHTED_AVERAGE  # when used with continuous coordinates, average features in the same coordinate\n",
    "    )\n",
    "    return stensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "noise = torch.randn(4, 3, 2048).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xs = to_parse_tensor(coords=noise, feats=noise, voxel_size=0.001)\n",
    "xs = ME.MinkowskiToSparseTensor()(noise)\n",
    "xs.F.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_gen = ME.MinkowskiConvolution(kernel_size=3, in_channels=3, out_channels=64, dimension=1).cuda()\n",
    "pool = ME.MinkowskiMaxPooling(kernel_size=4, stride=4, dimension=1).cuda()\n",
    "\n",
    "print(xs.F.shape)\n",
    "sfeats = feat_gen(xs)\n",
    "print(sfeats.F.shape)\n",
    "sfeats = pool(sfeats)\n",
    "print(sfeats.F.shape)\n",
    "feats = sfeats.decomposed_features\n",
    "\n",
    "feats = torch.cat([f.unsqueeze(0) for f in feats], dim=0)\n",
    "feats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# voxelization\n",
    "from modules.voxelization import Voxelization\n",
    "from modules import functional as F\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testhr = torch.rand(1, 3, 2048).cuda()\n",
    "testlr = testhr[:, :, ::128]\n",
    "\n",
    "resolution = 4\n",
    "\n",
    "vox = Voxelization(resolution=resolution, normalize=True, eps=0)\n",
    "\n",
    "\n",
    "voxfhr, norm_coords_hr = vox(features=testhr, coords=testhr)\n",
    "voxflr, norm_coords_lr = vox(features=testlr, coords=testlr)\n",
    "\n",
    "devoxhr = F.trilinear_devoxelize(voxfhr, norm_coords_hr, resolution)\n",
    "devoxlr = F.trilinear_devoxelize(voxflr, norm_coords_hr, resolution)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voxfhr.shape, norm_coords_hr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureVoxelConcatenation(nn.module):\n",
    "    def __init__(self, resolution):\n",
    "        super().__init__()\n",
    "        self.resolution = resolution\n",
    "        self.vox = Voxelization(resolution=resolution, normalize=True, eps=0)\n",
    "\n",
    "    def forward(self, x1_features, x2_features, x1_coords, x2_coords):\n",
    "        vox_x1, nc_x1 = self.vox(features=x1_features, coords=x1_coords)\n",
    "        vox_x2, nc_x2 = self.vox(features=x2_features, coords=x2_coords)\n",
    "\n",
    "        devox_mixed = F.trilinear_devoxelize(vox_x2, nc_x1, self.resolution)\n",
    "\n",
    "        return torch.cat([x1_features, devox_mixed], dim=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Schroedinger Bridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import open3d as o3d\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ply_to_np(pcd):\n",
    "    \"\"\"Converts a ply file to a numpy array with points and colors\"\"\"\n",
    "    points = np.asarray(pcd.points)\n",
    "    colors = np.asarray(pcd.colors)\n",
    "    stacked = np.hstack((points, colors))\n",
    "    return stacked\n",
    "\n",
    "def apply_transform(array, transformation):\n",
    "    \"\"\"Transforms a numpy array of points with a transformation matrix\"\"\"\n",
    "    ones = np.ones((array.shape[0], 1))\n",
    "    stacked = np.hstack((array, ones))\n",
    "    transformed = np.dot(transformation, stacked.T)\n",
    "    transformed = transformed.T[..., :3]\n",
    "    return transformed\n",
    "\n",
    "def inverse_T(transformation):\n",
    "    R = transformation[:3, :3]\n",
    "    t = transformation[:3, 3]\n",
    "    inv = np.zeros((4, 4))\n",
    "    inv[:3, :3] = R.T\n",
    "    inv[:3, 3] = -R.T @ t\n",
    "    inv[3, 3] = 1\n",
    "    return inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"../datasets/arkit/\"\n",
    "\n",
    "# specific paths\n",
    "arkit_ply = root + \"42445028_3dod_mesh.ply\"\n",
    "transformation_matrix = root + \"42445028_estimated_transform.npy\"\n",
    "hires_ply = root + \"42445028_highres_recon.ply\"\n",
    "faro_ply = root + \"421378.ply\"\n",
    "textured_root = root + \"textured.obj\"\n",
    "\n",
    "# load data\n",
    "arkit_pcd = o3d.io.read_point_cloud(arkit_ply)\n",
    "T_faro_arkit = np.load(transformation_matrix)\n",
    "hires_pcd = o3d.io.read_point_cloud(hires_ply)\n",
    "faro_pcd = o3d.io.read_point_cloud(faro_ply)\n",
    "\n",
    "# downsample the pointsclouds\n",
    "arkit_pcd = arkit_pcd.voxel_down_sample(voxel_size=0.03)\n",
    "faro_pcd = faro_pcd.voxel_down_sample(voxel_size=0.03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_arkit_faro = inverse_T(T_faro_arkit)\n",
    "arkit_npy = ply_to_np(arkit_pcd)[..., :3]\n",
    "hires_npy = ply_to_np(hires_pcd)[..., :3]\n",
    "faro_npy = ply_to_np(faro_pcd)[..., :3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faro_npy_transformed = apply_transform(faro_npy[..., :3], T_faro_arkit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics.emd_ import emd_module\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emd = emd_module.emdModule()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "highres = torch.tensor(faro_npy_transformed).cuda().unsqueeze(0)\n",
    "lowres = torch.tensor(arkit_npy).cuda().unsqueeze(0)\n",
    "# shuffle highres cloud\n",
    "highres = highres[:, torch.randperm(highres.shape[1]), :]\n",
    "lowres.shape, highres.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pts_lowres = lowres.shape[1]\n",
    "highres_downsampled = highres[:, :n_pts_lowres, :]\n",
    "\n",
    "# make points divisible by 128\n",
    "remainder = n_pts_lowres % 128\n",
    "if remainder != 0:\n",
    "    lowres = lowres[:, :-remainder, :]\n",
    "    highres_downsampled = highres_downsampled[:, :-remainder, :]\n",
    "\n",
    "highres_downsampled.shape, lowres.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    dist, assignment = emd(highres_downsampled, lowres, eps=0.001, iters=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "assignment.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.sqrt(dist).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowres_aligned = lowres.gather(dim=1, index=assignment.type(torch.int64).unsqueeze(-1).expand(-1, -1, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowres_aligned.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dists = torch.sqrt(torch.sum((highres_downsampled - lowres_aligned)**2, dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_std = dists.std(dim=1).mean()\n",
    "d_filter = dists > 2*d_std\n",
    "d_std, d_filter.sum() / dists.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot points in 2d including their mapping shown by a line\n",
    "idx_start = 20_000\n",
    "length = 1000\n",
    "\n",
    "p1 = lowres_aligned[:, idx_start:idx_start+length, :]\n",
    "p2 = highres_downsampled[:, idx_start:idx_start+length, :]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "idx1 = 2\n",
    "idx2 = 0\n",
    "\n",
    "plt.close(\"all\")\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.scatter(p1[0, :, idx1].cpu().numpy(), p1[0, :, idx2].cpu().numpy(), s=1, color=\"blue\")\n",
    "ax.scatter(p2[0, :, idx1].cpu().numpy(), p2[0, :, idx2].cpu().numpy(), s=1, color=\"red\")\n",
    "\n",
    "# show lines of corresponding points\n",
    "for i in range(p1.shape[1]):\n",
    "    ax.plot([p1[0, i, idx1].cpu().numpy(), p2[0, i, idx1].cpu().numpy()], [p1[0, i, idx2].cpu().numpy(), p2[0, i, idx2].cpu().numpy()], color=\"black\", alpha=0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"I2SB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from I2SB.i2sb.diffusion import Diffusion\n",
    "from I2SB.i2sb.runner import make_beta_schedule\n",
    "\n",
    "steps = 200\n",
    "betas = make_beta_schedule(n_timestep=steps, linear_end=1e-4)\n",
    "betas = np.concatenate([betas[:steps//2], np.flip(betas[:steps//2])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffusion = Diffusion(betas=betas, device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qsamps = []\n",
    "\n",
    "for t in range(steps):\n",
    "    qsamp = diffusion.q_sample(step=t, x0=p2, x1=p1, ot_ode=True).float()\n",
    "    qsamps.append(qsamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the samples as trajectory projected on x y and z planes\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "for samp in qsamps:\n",
    "    plt.scatter(samp[0, :, 0].cpu().numpy(), samp[0, :, 1].cpu().numpy(), color=\"blue\", alpha=0.1)\n",
    "plt.show()\n",
    "\n",
    "for samp in qsamps:\n",
    "    plt.scatter(samp[0, :, 0].cpu().numpy(), samp[0, :, 2].cpu().numpy(), color=\"blue\", alpha=0.1)\n",
    "plt.show()\n",
    "\n",
    "for samp in qsamps:\n",
    "    plt.scatter(samp[0, :, 1].cpu().numpy(), samp[0, :, 2].cpu().numpy(), color=\"blue\", alpha=0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diffusion Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export CUDA_HOME=/usr/local/cuda-11.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.dataloader import get_dataloader, save_iter\n",
    "from omegaconf import OmegaConf\n",
    "from model.loader import load_diffusion\n",
    "from utils.utils import get_data_batch, smart_load_model_weights, to_cuda\n",
    "import os\n",
    "\n",
    "# set CUDA_LAUNCH_BLOCKING=1\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = \"checkpoints/snpp_dino_iphone_no_ote/opt.yaml\"\n",
    "cfg = OmegaConf.load(cfg)\n",
    "\n",
    "cfg.data.data_dir = \"/mnt/d/ML/datasets/snpp_processed//\"\n",
    "\n",
    "cfg.distribution_type = \"single\"\n",
    "cfg.gpu = None\n",
    "cfg.model_path = \"\"\n",
    "cfg.diffusion.ot_ode = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, train_sampler, val_sampler = get_dataloader(cfg, sampling=True)\n",
    "model, ckpt = load_diffusion(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "steps = np.linspace(0, 399, 400)\n",
    "steps = steps.astype(int).tolist()\n",
    "steps = torch.tensor(steps).long()\n",
    "\n",
    "# add labels\n",
    "plt.plot(steps, model.mu_x0[steps].cpu())\n",
    "plt.plot(steps, model.mu_x1[steps].cpu())\n",
    "plt.plot(steps, model.std_sb[steps].cpu())\n",
    "plt.legend([\"mux0\", \"mux1\", \"stdsb\"])\n",
    "plt.xlabel(\"Step\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = save_iter(train_loader, train_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = to_cuda(batch, \"cuda\")\n",
    "data_batch = get_data_batch(batch=batch, cfg=cfg)\n",
    "\n",
    "x0b = data_batch[\"hr_points\"]\n",
    "x1b = data_batch[\"lr_points\"] if data_batch[\"lr_points\"] is not None else None\n",
    "featuresb = data_batch[\"features\"] if data_batch[\"features\"] is not None else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "outs = []\n",
    "outs_fixed = []\n",
    "\n",
    "outs.append(x0b.cpu().numpy())\n",
    "\n",
    "for t in range(0, cfg.diffusion.timesteps, 10):\n",
    "    # make t batch size\n",
    "    t = torch.tensor(t).reshape(1,).long()\n",
    "    t = t.repeat(x0b.shape[0])\n",
    "    out = model.q_sample(x1=x1b, x0=x0b, step=t).squeeze()\n",
    "    outs.append(out.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "outs = np.array(outs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bidx = 7\n",
    "o = outs[:, bidx, :, :]\n",
    "x0 = x0b[bidx:bidx+1, :, :]\n",
    "x1 = x1b[bidx:bidx+1, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plot x0 and x1 and the outs as trajectory in betwen them, plot 3d\n",
    "# outs have shape (3 x N)\n",
    "\n",
    "ax = plt.figure(figsize=(10, 10)).add_subplot(projection='3d')\n",
    "ax.scatter(x0[0, 0, :].cpu().numpy(), x0[0, 1, :].cpu().numpy(), zs=x0[0, 2, :].cpu().numpy(), color=\"blue\", alpha=1)\n",
    "ax.scatter(x1[0, 0, :].cpu().numpy(), x1[0, 1, :].cpu().numpy(), zs=x1[0, 2, :].cpu().numpy(), color=\"red\", alpha=1)\n",
    "\n",
    "# for each point in outs, plot the trajectory in 3d, outs has shape (T, 3, P)\n",
    "# where P are the points\n",
    "for i in range(o.shape[2]):\n",
    "    ax.plot(o[:, 0, i], o[:, 1, i], zs=o[:, 2, i], color=\"black\", alpha=1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a gif of whole pointcloud moving from x1 to x0 over the outs\n",
    "import imageio\n",
    "import os\n",
    "\n",
    "images = []\n",
    "\n",
    "for i in range(o.shape[0]):\n",
    "    ptc = o[::-1][i, :, :].T\n",
    "    ax = plt.figure(figsize=(10, 10)).add_subplot(projection='3d')\n",
    "    ax.scatter(ptc[:, 0], ptc[:, 1], zs=ptc[:, 2], color=\"blue\", alpha=1)\n",
    "    # fix the axis siz\n",
    "    ax.set_xlim(-1, 1)\n",
    "    ax.set_ylim(-1, 1)\n",
    "    ax.set_zlim(-1, 1)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"img_{i}.png\")\n",
    "    plt.close()\n",
    "    images.append(imageio.imread(f\"img_{i}.png\"))\n",
    "    os.remove(f\"img_{i}.png\")\n",
    "\n",
    "imageio.mimsave(\"out.mp4\", images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "test = torch.ones(4, 3, 4).cuda()\n",
    "steps = [1, 100, 500, 800]\n",
    "test_out = model.q_sample(x1=test, x0=torch.zeros_like(test), step=steps, ot_ode=True)\n",
    "test_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyviz3d.visualizer import Visualizer\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "viz = Visualizer()\n",
    "\n",
    "test_room = \"../test_room/02455b3d20/\"\n",
    "model = \"snpp_rgb_iphone\"\n",
    "\n",
    "gt = test_room + f\"gt_{model}.npy\"\n",
    "denoised = test_room + f\"denoised_{model}.npy\"\n",
    "noised = test_room + f\"noised_{model}.npy\"\n",
    "\n",
    "gt = np.load(gt)\n",
    "denoised = np.load(denoised)\n",
    "noised = np.load(noised)\n",
    "\n",
    "viz.add_points(\"gt\", gt, point_size=1, visible=False)\n",
    "viz.add_points(\"denoised\", denoised, point_size=1, visible=True)\n",
    "viz.add_points(\"noised\", noised, point_size=1, visible=False)\n",
    "\n",
    "viz.save(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyviz3d.visualizer as viz\n",
    "import numpy as np\n",
    "import open3d as o3d\n",
    "\n",
    "old = \"/mnt/d/ML/datasets/scannet++/data/02455b3d20/scans/iphone.ply\"\n",
    "v2 = \"/mnt/d/ML/datasets/scannet++/data/02455b3d20/scans/iphone_v2.ply\" # ok\n",
    "v3 = \"/mnt/d/ML/datasets/scannet++/data/02455b3d20/scans/iphone_v3.ply\" # ok => --grid_size 0.05 --n_outliers 20 --outlier_radius 0.05 --final_grid_size 0.02 --final_n_outliers 10 --final_outlier_radius 0.05 \n",
    "v4 = \"/mnt/d/ML/datasets/scannet++/data/02455b3d20/scans/iphone_v4.ply\" # shit\n",
    "v5 = \"/mnt/d/ML/datasets/scannet++/data/02455b3d20/scans/iphone_v5.ply\" # shit\n",
    "v3_1 = \"/mnt/d/ML/datasets/scannet++/data/02455b3d20/scans/iphone_v3.1.ply\"\n",
    "\n",
    "old = np.array(o3d.io.read_point_cloud(old).points)\n",
    "v2 = np.array(o3d.io.read_point_cloud(v2).points)\n",
    "v3 = np.array(o3d.io.read_point_cloud(v3).points)\n",
    "v4 = np.array(o3d.io.read_point_cloud(v4).points)\n",
    "v5 = np.array(o3d.io.read_point_cloud(v5).points)\n",
    "v3_1 = np.array(o3d.io.read_point_cloud(v3_1).points)\n",
    "\n",
    "viz = viz.Visualizer()\n",
    "viz.add_points(\"old\", old, point_size=1, visible=False)\n",
    "viz.add_points(\"v2\", v2, point_size=1, visible=False)\n",
    "viz.add_points(\"v3\", v3, point_size=1, visible=False)\n",
    "viz.add_points(\"v4\", v4, point_size=1, visible=False)\n",
    "viz.add_points(\"v5\", v5, point_size=1, visible=False)\n",
    "viz.add_points(\"3.1\", v3_1, point_size=1, visible=False)\n",
    "\n",
    "viz.save(\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PointTransformer V3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training.pointtransformer_v3 import PointTransformerV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "model = PointTransformerV3(\n",
    "    in_channels=370,\n",
    "    time_dim=1,\n",
    "    order=[\"z\", \"z-trans\", \"hilbert\", \"hilbert-trans\"],\n",
    "    stride=(2, 2, 2, 2),\n",
    "    enc_depths=(2, 2, 2, 6, 2),\n",
    "    enc_channels=(128, 128, 256, 256, 512),\n",
    "    enc_num_head=(2, 4, 8, 16, 32),\n",
    "    enc_patch_size=(1024, 1024, 1024, 1024, 1024),\n",
    "    dec_depths=(1, 1, 1, 1),\n",
    "    dec_channels=(32, 64, 128, 256),\n",
    "    dec_num_head=(4, 4, 8, 16),\n",
    "    dec_patch_size=(1024, 1024, 1024, 1024),\n",
    "    enable_flash=True,\n",
    "    qkv_bias=True,\n",
    "    upcast_attention=False,\n",
    "    upcast_softmax=False,\n",
    ").cuda()\n",
    "\n",
    "\n",
    "test_input = torch.randn(4, 370, 4096).cuda()\n",
    "test_timestep = torch.randn(4, 1).cuda()\n",
    "\n",
    "with torch.cuda.amp.autocast():\n",
    "    out = model(test_input, test_timestep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'open3d.cuda.pybind.geometry.PointCloud' object has no attribute 'uniform_downsample'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m test \u001b[38;5;241m=\u001b[39m root \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscans/test.ply\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     10\u001b[0m scan_points \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(o3d\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mread_point_cloud(scan)\u001b[38;5;241m.\u001b[39mpoints)\n\u001b[0;32m---> 11\u001b[0m faro_points \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[43mo3d\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_point_cloud\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfaro\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvoxel_down_sample\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muniform_downsample\u001b[49m(\u001b[38;5;241m0.9\u001b[39m)\u001b[38;5;241m.\u001b[39mpoints)\n\u001b[1;32m     12\u001b[0m test_points \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(o3d\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mread_point_cloud(test)\u001b[38;5;241m.\u001b[39mpoints)\n\u001b[1;32m     14\u001b[0m scan_points\u001b[38;5;241m.\u001b[39mshape, faro_points\u001b[38;5;241m.\u001b[39mshape, test_points\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'open3d.cuda.pybind.geometry.PointCloud' object has no attribute 'uniform_downsample'"
     ]
    }
   ],
   "source": [
    "import open3d as o3d\n",
    "import numpy as np\n",
    "from utils.utils import filter_iphone_scan\n",
    "\n",
    "root = \"/mnt/d/ML/datasets/scannet++/data/02455b3d20/\"\n",
    "scan = root + \"scans/iphone_v2.ply\"\n",
    "faro = root + \"scans/mesh_aligned_0.05.ply\"\n",
    "test = root + \"scans/test.ply\"\n",
    "\n",
    "faro = o3d.io.read_point_cloud(faro).voxel_down_sample(0.01)\n",
    "\n",
    "scan_points = np.array(o3d.io.read_point_cloud(scan).points)\n",
    "faro_points = np.array(o3d.io.read_point_cloud(faro).voxel_down_sample(0.01).points)\n",
    "test_points = np.array(o3d.io.read_point_cloud(test).points)\n",
    "\n",
    "scan_points.shape, faro_points.shape, test_points.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered = filter_iphone_scan(scan_points, faro_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(124108, 3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************************************************************************\n",
      "1) Start local server:\n",
      "    cd /home/mathi/DPU/PCUpsampling/test; python -m http.server 6008\n",
      "2) Open in browser:\n",
      "    http://0.0.0.0:6008\n",
      "************************************************************************\n"
     ]
    }
   ],
   "source": [
    "from pyviz3d.visualizer import Visualizer\n",
    "\n",
    "v = Visualizer()\n",
    "\n",
    "v.add_points(\"iphone\", scan_points, point_size=1, visible=False)\n",
    "v.add_points(\"faro\", faro_points, point_size=1, visible=False)\n",
    "v.add_points(\"filtered\", filtered, point_size=1, visible=True)\n",
    "v.add_points(\"test\", test_points, point_size=1, visible=False)\n",
    "\n",
    "\n",
    "v.save(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import cKDTree\n",
    "import numpy as np\n",
    "from pvcnn.functional.sampling import furthest_point_sample\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 4096\n",
    "\n",
    "n_batches = faro_points.shape[0] // bs\n",
    "\n",
    "centers = furthest_point_sample(torch.tensor(faro_points.T).unsqueeze(0).cuda().float(), bs).squeeze().cpu().numpy().T\n",
    "\n",
    "faro_tree = cKDTree(faro_points)\n",
    "filtered_tree = cKDTree(filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2332 1024\n",
      "19110 1024\n",
      "8708 1024\n",
      "9916 1024\n",
      "18497 1024\n",
      "7370 1024\n",
      "8131 1024\n",
      "7269 1024\n",
      "12639 1024\n",
      "4118 1024\n",
      "10454 1024\n",
      "7524 1024\n",
      "9675 1024\n",
      "6171 1024\n",
      "5162 1024\n",
      "6683 1024\n",
      "9407 1024\n",
      "5924 1024\n",
      "6215 1024\n",
      "7963 1024\n",
      "10504 1024\n",
      "8397 1024\n",
      "19669 1024\n",
      "11476 1024\n",
      "11838 1024\n",
      "4144 1024\n",
      "22164 1024\n",
      "7103 1024\n",
      "11239 1024\n",
      "2045 1024\n",
      "17882 1024\n",
      "5708 1024\n"
     ]
    }
   ],
   "source": [
    "for center in centers[:32]:\n",
    "    dists_lr, nbs_lr = filtered_tree.query(center, k=1024)\n",
    "    max_dist_lr = dists_lr.max() \n",
    "    nbs_hr = faro_tree.query_ball_point(center, max_dist_lr)\n",
    "    print(len(nbs_hr), len(nbs_lr))\n",
    "    if len(nbs_lr) > 4096:\n",
    "        print(\"error\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dpmpc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
