{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.cuda.amp import autocast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.pvcnn_generation import PVCNNFeatures\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PVCNNFeatures(input_dim=3, extra_feature_channels=0).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = torch.randn(3, 3, 2048*8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = model(test.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in feats:\n",
    "    print(f.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.recurrent_interface_network import RIN\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RIN(\n",
    "    dim = 256,                  # model dimensions\n",
    "    image_size = 128,           # image size\n",
    "    patch_size = 8,             # patch size\n",
    "    depth = 6,                  # depth\n",
    "    num_latents = 128,          # number of latents. they used 256 in the paper\n",
    "    dim_latent = 512,           # can be greater than the image dimension (dim) for greater capacity\n",
    "    latent_self_attn_depth = 4, # number of latent self attention blocks per recurrent step, K in the paper\n",
    ").cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_images = torch.randn(8, 3, 128, 128).cuda()\n",
    "time = torch.rand(8).cuda()\n",
    "\n",
    "out = model(training_images, time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.recurrent_pointcloud_network import RINBlock, RIN\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block = RINBlock(pc_dim=3, latent_self_attn_depth=6, dim_latent=256)\n",
    "patches = torch.randn(3, 2048, 3)\n",
    "latents = torch.zeros(3, 768, 256)\n",
    "t = torch.randn(3)\n",
    "\n",
    "out = block(patches=patches, latents=latents, t=t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RIN(in_dim=6,out_dim=3, depth=6, latent_self_attn_depth=6, dim_latent=256, num_latents=768).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cloud = torch.randn(3, 2048, 3).cuda()\n",
    "test_time = torch.randn(3).cuda()\n",
    "out = model(test_cloud, test_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gridding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import open3d as o3d\n",
    "import numpy as np\n",
    "\n",
    "cloud = \"../3d/421378.ply\"\n",
    "\n",
    "pc = o3d.io.read_point_cloud(cloud)\n",
    "pcnpy = np.asarray(pc.points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcnpy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_cloud = pcnpy\n",
    "# substract center\n",
    "point_cloud -= np.mean(point_cloud, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.diffusion_lucid import GaussianDiffusion\n",
    "from omegaconf import OmegaConf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_path = \"configs/pvd_arkit.yml\"\n",
    "cfg = OmegaConf.load(cfg_path)\n",
    "cfg.diffusion.num_timesteps = 1000\n",
    "\n",
    "diffusion = GaussianDiffusion(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import point_cloud_utils as pcu\n",
    "import pyminiply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ply_path = \"../3d/mesh_aligned_0.05.ply\"\n",
    "ply, *_ = pyminiply.read(ply_path)\n",
    "pcd = ply\n",
    "pcd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.rand(10000, 3)\n",
    "b = np.random.rand(10000, 3)\n",
    "\n",
    "chamfer_dist = pcu.chamfer_distance(a, b)\n",
    "chamfer_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ply_path = \"../3d/mesh_aligned_0.05.ply\"\n",
    "ply = pcu.load_mesh_v(ply_path)\n",
    "pcd = ply\n",
    "pcd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "pcd_tree = scipy.spatial.cKDTree(ply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_idx = np.random.randint(0, len(ply))   \n",
    "k_nearest = pcd_tree.query(ply[rand_idx], 16000)[1]\n",
    "x = pcd[k_nearest]\n",
    "\n",
    "# plot point cloud\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(x[:, 0], x[:, 1], x[:, 2], s=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# plot influence of denoising timesteps\n",
    "timesteps = [10, 50, 100, 200, 1000]\n",
    "\n",
    "# get x0 and normalize to [-1, 1]\n",
    "x0 = x \n",
    "x0 = x0 - np.min(x0, axis=0)\n",
    "x0 = x0 / np.max(x0, axis=0)\n",
    "x0 = torch.from_numpy(x0).float().unsqueeze(0)\n",
    "\n",
    "epsilon = torch.randn_like(x0)\n",
    "\n",
    "\n",
    "for tglobal in timesteps:\n",
    "    cfg.diffusion.num_timesteps = tglobal\n",
    "    diffusion = GaussianDiffusion(cfg)\n",
    "    samples = torch.tensor([])\n",
    "    \n",
    "    for t in np.arange(0, tglobal, tglobal//10):\n",
    "        t = torch.tensor(t).reshape(1,).long()\n",
    "        qsample = diffusion.q_sample(x0, t=t, noise=epsilon)\n",
    "        samples = torch.cat((samples, qsample), dim=0)\n",
    "    \n",
    "    # plot samples next to each other\n",
    "    fig = plt.figure(figsize=(20, 10))\n",
    "    \n",
    "    # set title\n",
    "    fig.suptitle(f\"timesteps: {tglobal}\", fontsize=16)\n",
    "    for i in range(10):\n",
    "        samp = samples[i, :, :].detach().numpy()\n",
    "        ax = fig.add_subplot(2, 5, i+1, projection='3d')\n",
    "        ax.scatter(samp[:, 0], samp[:, 1], samp[:, 2], s=1)\n",
    "        # calculate mean and var\n",
    "        mean = np.mean(samp)\n",
    "        var = np.var(samp)\n",
    "        # set title\n",
    "        ax.set_title(f\"timestep: {i * tglobal//10} mean: {mean:.2f}, var: {var:.2f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.dataloader import get_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../datasets/scannetpp/Training'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/mathi/Desktop/DPU/PCUpsampling/tester.ipynb Cell 33\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/mathi/Desktop/DPU/PCUpsampling/tester.ipynb#X44sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m cfg \u001b[39m=\u001b[39m OmegaConf\u001b[39m.\u001b[39mload(cfg_path)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/mathi/Desktop/DPU/PCUpsampling/tester.ipynb#X44sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m cfg\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mdata_dir \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m../datasets/scannetpp/\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/mathi/Desktop/DPU/PCUpsampling/tester.ipynb#X44sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m train_loader, val_loader, \u001b[39m*\u001b[39m_ \u001b[39m=\u001b[39m get_dataloader(cfg)\n",
      "File \u001b[0;32m~/Desktop/DPU/PCUpsampling/data/dataloader.py:56\u001b[0m, in \u001b[0;36mget_dataloader\u001b[0;34m(opt, sampling)\u001b[0m\n\u001b[1;32m     52\u001b[0m     train_dataset \u001b[39m=\u001b[39m IndoorScenesCut(\n\u001b[1;32m     53\u001b[0m         opt\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mdata_dir, opt\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mnpoints, voxel_size\u001b[39m=\u001b[39mopt\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mvoxel_size, normalize\u001b[39m=\u001b[39mopt\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mnormalize\n\u001b[1;32m     54\u001b[0m     )\n\u001b[1;32m     55\u001b[0m \u001b[39melif\u001b[39;00m opt\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mdataset \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mArkit\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m---> 56\u001b[0m     train_dataset \u001b[39m=\u001b[39m ArkitScans(\n\u001b[1;32m     57\u001b[0m         os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(opt\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mdata_dir, \u001b[39m\"\u001b[39;49m\u001b[39mTraining\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m     58\u001b[0m         opt\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mnpoints,\n\u001b[1;32m     59\u001b[0m         voxel_size\u001b[39m=\u001b[39;49mopt\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mvoxel_size,\n\u001b[1;32m     60\u001b[0m         normalize\u001b[39m=\u001b[39;49mopt\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mnormalize,\n\u001b[1;32m     61\u001b[0m         unconditional\u001b[39m=\u001b[39;49mopt\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49munconditional,\n\u001b[1;32m     62\u001b[0m     )\n\u001b[1;32m     63\u001b[0m \u001b[39melif\u001b[39;00m opt\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mdataset \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mScannetPP\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m     64\u001b[0m     train_dataset \u001b[39m=\u001b[39m ScanNetPPCut(npoints\u001b[39m=\u001b[39mopt\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mnpoints, root\u001b[39m=\u001b[39mopt\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mdata_dir, mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtraining\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/DPU/PCUpsampling/data/arkitscenes.py:193\u001b[0m, in \u001b[0;36mArkitScans.__init__\u001b[0;34m(self, root_dir, npoints, voxel_size, normalize, unconditional)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munconditional \u001b[39m=\u001b[39m unconditional\n\u001b[1;32m    192\u001b[0m \u001b[39m# specific paths\u001b[39;00m\n\u001b[0;32m--> 193\u001b[0m folders \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39;49mlistdir(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mroot)\n\u001b[1;32m    194\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mSetting up arkit scans dataset\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    195\u001b[0m folders \u001b[39m=\u001b[39m [f \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m folders \u001b[39mif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39misdir(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mroot, f))]\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../datasets/scannetpp/Training'"
     ]
    }
   ],
   "source": [
    "from omegaconf import OmegaConf\n",
    "\n",
    "cfg_path = \"configs/pvd_arkit.yml\"\n",
    "cfg = OmegaConf.load(cfg_path)\n",
    "cfg.data.data_dir = \"../datasets/scannetpp/\"\n",
    "cfg.data.dataset = \"ScannetPP\"\n",
    "\n",
    "\n",
    "train_loader, val_loader, *_ = get_dataloader(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(ds, batch_size=1, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(dataloader))\n",
    "points = batch[\"train_points\"]\n",
    "\n",
    "# visualize point cloud\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(points[0, :, 0], points[0, :, 1], points[0, :, 2], s=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.dataloader import get_dataloader\n",
    "from omegaconf import OmegaConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = \"configs/overfit_room.yml\"\n",
    "opt = OmegaConf.load(conf)\n",
    "opt.data.dataset = \"Indoor\"\n",
    "opt.data.npoints = 2**14\n",
    "opt.distribution_type = \"single\"\n",
    "dl, *_ = get_dataloader(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter((dl)))\n",
    "data = batch[\"train_points\"]\n",
    "lowres = batch[\"train_points_lowres\"]\n",
    "data.shape, data.min(), data.max(), data.mean(), data.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyviz3d.visualizer as viz\n",
    "\n",
    "ps = 2\n",
    "\n",
    "v = viz.Visualizer()\n",
    "v.add_points(\"hires\", data[1, ...].numpy(), point_size=ps, visible=False)\n",
    "v.add_points(\"lowres\", lowres[1, ...].numpy(), point_size=ps, visible=True)\n",
    "v.save(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.utils import concat_nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "x1 = torch.randn(100000, 3)\n",
    "x2 = torch.randn(100000, 3)\n",
    "y = concat_nn(x1, x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PVCLion(out_dim=3, input_dim=3, embed_dim=16, npoints=1_000_000, use_att=False, extra_feature_channels=3, dropout=0.1).cuda()\n",
    "# print number of parameters\n",
    "print(f\"Number of parameters: {sum(p.numel() for p in model.parameters()) / 1_000_000}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DotDict(dict):\n",
    "    \"\"\"\n",
    "    a dictionary that supports dot notation \n",
    "    as well as dictionary access notation \n",
    "    usage: d = DotDict() or d = DotDict({'val1':'first'})\n",
    "    set attributes: d.val2 = 'second' or d['val2'] = 'second'\n",
    "    get attributes: d.val2 or d['val2']\n",
    "    \"\"\"\n",
    "    __getattr__ = dict.__getitem__\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__\n",
    "\n",
    "    def __init__(self, dct):\n",
    "        for key, value in dct.items():\n",
    "            if hasattr(value, 'keys'):\n",
    "                value = DotDict(value)\n",
    "            self[key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from minkowski.minkunet import ResUNet14, TinyUnet\n",
    "\n",
    "config = DotDict\n",
    "config.bn_momentum = 0.9\n",
    "config.conv1_kernel_size = 7\n",
    "\n",
    "model = TinyUnet(in_channels=3, out_channels=3, config=config, D=3).cuda()\n",
    "print(f\"Number of parameters: {sum(p.numel() for p in model.parameters()) / 1_000_000}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = IndoorScenes(root_dir=\"../../3d/\", npoints=1_000_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ds[0]\n",
    "\n",
    "pcond = data[\"train_points_lowres\"].unsqueeze(0).cuda()\n",
    "noise = torch.randn(1, 1_000_000, 3).cuda()\n",
    "t = torch.rand(1).cuda()\n",
    "\n",
    "# concat\n",
    "#x = torch.cat([pcond, noise], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MinkowskiEngine as ME\n",
    "\n",
    "def to_parse_tensor(coords, feats=None, voxel_size=0.001):\n",
    "    \"\"\"\n",
    "    Converts the input coordinates and features to a sparse tensor.\n",
    "\n",
    "    Args:\n",
    "        coords (torch.Tensor): The input coordinates.\n",
    "        feats (torch.Tensor): The input features.\n",
    "        voxel_size (float): The voxel size for scaling the coordinates.\n",
    "\n",
    "    Returns:\n",
    "        ME.SparseTensor: The sparse tensor with the converted coordinates and features.\n",
    "    \"\"\"\n",
    "    # scaling\n",
    "    coords = coords / voxel_size\n",
    "    b, n, c = coords.shape\n",
    "    \n",
    "    if feats is None:\n",
    "        feats = torch.zeros((b*n, 3)).to(coords.device)\n",
    "    elif feats.ndim == 3:\n",
    "        feats = feats.reshape(b*n, 3)\n",
    "\n",
    "    stensor = ME.SparseTensor(\n",
    "        features=feats, # Convert to a tensor\n",
    "        coordinates=ME.utils.batched_coordinates([c for c in coords], dtype=torch.float32, device=coords.device),  # coordinates must be defined in a integer grid. If the scale\n",
    "        quantization_mode=ME.SparseTensorQuantizationMode.UNWEIGHTED_AVERAGE  # when used with continuous coordinates, average features in the same coordinate\n",
    "    )\n",
    "    return stensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = to_parse_tensor(coords=noise, feats=noise, voxel_size=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# voxelization\n",
    "from modules.voxelization import Voxelization\n",
    "from modules import functional as F\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testhr = torch.rand(1, 3, 2048).cuda()\n",
    "testlr = testhr[:, :, ::128]\n",
    "\n",
    "resolution = 4\n",
    "\n",
    "vox = Voxelization(resolution=resolution, normalize=True, eps=0)\n",
    "\n",
    "\n",
    "voxfhr, norm_coords_hr = vox(features=testhr, coords=testhr)\n",
    "voxflr, norm_coords_lr = vox(features=testlr, coords=testlr)\n",
    "\n",
    "devoxhr = F.trilinear_devoxelize(voxfhr, norm_coords_hr, resolution)\n",
    "devoxlr = F.trilinear_devoxelize(voxflr, norm_coords_hr, resolution)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voxfhr.shape, norm_coords_hr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureVoxelConcatenation(nn.module):\n",
    "    def __init__(self, resolution):\n",
    "        super().__init__()\n",
    "        self.resolution = resolution\n",
    "        self.vox = Voxelization(resolution=resolution, normalize=True, eps=0)\n",
    "\n",
    "    def forward(self, x1_features, x2_features, x1_coords, x2_coords):\n",
    "        vox_x1, nc_x1 = self.vox(features=x1_features, coords=x1_coords)\n",
    "        vox_x2, nc_x2 = self.vox(features=x2_features, coords=x2_coords)\n",
    "\n",
    "        devox_mixed = F.trilinear_devoxelize(vox_x2, nc_x1, self.resolution)\n",
    "\n",
    "        return torch.cat([x1_features, devox_mixed], dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "devoxlr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voxflr.shape, voxfhr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voxchr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vox.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with autocast(cache_enabled=False):\n",
    "    out = model(inputs=x, t=t)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torchsparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "from torchsparse import nn as spnn\n",
    "from torchsparse.utils.quantize import sparse_quantize\n",
    "from torchsparse.utils.collate import sparse_collate_fn\n",
    "\n",
    "from torchsparse import SparseTensor\n",
    "\n",
    "class TestModel(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "    \n",
    "        self.net = nn.Sequential(\n",
    "            spnn.Conv3d(3, 64, 3),\n",
    "            spnn.BatchNorm(64),\n",
    "            spnn.ReLU(True),\n",
    "            nn.Linear(64, 64),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        return x\n",
    "\n",
    "model = nn.Sequential(\n",
    "        spnn.Conv3d(3, 32, 3),\n",
    "        spnn.BatchNorm(32),\n",
    "        spnn.ReLU(True),\n",
    "        spnn.Conv3d(32, 64, 2, stride=2),\n",
    "        spnn.BatchNorm(64),\n",
    "        spnn.ReLU(True),\n",
    "        spnn.Conv3d(64, 64, 2, stride=2, transposed=True),\n",
    "        spnn.BatchNorm(64),\n",
    "        spnn.ReLU(True),\n",
    "        spnn.Conv3d(64, 32, 3),\n",
    "        spnn.BatchNorm(32),\n",
    "        spnn.ReLU(True),\n",
    "        spnn.Conv3d(32, 10, 1),\n",
    "    ).to(\"cuda\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_in = torch.randn(2048, 3).numpy()\n",
    "coords, indices = sparse_quantize(test_in, 1e-3, return_index=True)\n",
    "coords = torch.tensor(coords, dtype=torch.int)\n",
    "feats = torch.tensor(test_in[indices], dtype=torch.float)\n",
    "tensor = SparseTensor(coords=coords, feats=feats)\n",
    "\n",
    "model(tensor)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dpmpc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
